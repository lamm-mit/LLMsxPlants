{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ff8e1b-6969-4972-96c6-d2709be2c023",
   "metadata": {},
   "source": [
    "# BioLLM x Plants - Idea Mining, Divergent-Convergent Script\n",
    "\n",
    "Rachel K. Luu, Ming Dao, Subra Suresh, Markus J. Buehler (2025) [full reference to be updated to be included here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ea25e",
   "metadata": {},
   "source": [
    "## Divergent Generation\n",
    "For the divergent generation phase, BioinspiredLLM quantized to 8bit is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e359c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate, SimpleDirectoryReader, VectorStoreIndex, Settings\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "from typing import List, Optional, Sequence\n",
    "import pandas as pd\n",
    "from utils.formats import messages_to_prompt, completion_to_prompt\n",
    "\n",
    "url = \"https://huggingface.co/rachelkluu/Llama3.1-8b-Instruct-CPT-SFT-DPO-09022024-Q8_0-GGUF/resolve/main/llama3.1-8b-instruct-cpt-sft-dpo-09022024-q8_0.gguf\"\n",
    "bioinspiredllm_q8 = LlamaCPP(\n",
    "    model_url=url,\n",
    "    model_path=None,\n",
    "    temperature=1,\n",
    "    max_new_tokens=2048,\n",
    "    context_window=16000,\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=False,  \n",
    ")\n",
    "\n",
    "Settings.llm = bioinspiredllm_q8 \n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"./RAG/\" \n",
    ").load_data()\n",
    "\n",
    "Settings.chunk_size = 128\n",
    "Settings.chunk_overlap = 50\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = vector_index.as_query_engine(response_mode=\"compact\", similarity_top_k=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb55b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What are other material applications that pollen grains can be used for?\"\n",
    "num_per_gen = \"\" #can optionally specify how many ideas per generation\n",
    "sim_thres = 0.7 #adjusts the similarity threshold, with values closer to 1 being greater similarity \n",
    "num_ideas = 100 #number of ideas desired in total\n",
    "csv_filename = \"divergentideas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c356fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 Ideas were generated\n",
      "Unique rows were added. Current number of rows: 11\n",
      "15 Ideas were generated\n",
      "Unique rows were added. Current number of rows: 26\n",
      "9 Ideas were generated\n",
      "Unique rows were added. Current number of rows: 35\n",
      "79 Ideas were generated\n",
      "Unique rows were added. Current number of rows: 66\n",
      "71 Ideas were generated\n",
      "Unique rows were added. Current number of rows: 79\n",
      "17 Ideas were generated\n",
      "Unique rows were added. Current number of rows: 94\n",
      "25 Ideas were generated\n",
      "Unique rows were added. Current number of rows: 114\n",
      "                                                Prompt  \\\n",
      "0    What are other material applications that poll...   \n",
      "1    What are other material applications that poll...   \n",
      "2    What are other material applications that poll...   \n",
      "3    What are other material applications that poll...   \n",
      "4    What are other material applications that poll...   \n",
      "..                                                 ...   \n",
      "109  What are other material applications that poll...   \n",
      "110  What are other material applications that poll...   \n",
      "111  What are other material applications that poll...   \n",
      "112  What are other material applications that poll...   \n",
      "113  What are other material applications that poll...   \n",
      "\n",
      "                                                  Idea  \n",
      "0    Natural dyes: Pollen grains can be used as a n...  \n",
      "1    Bioplastics: Pollen-based bioplastics can be d...  \n",
      "2    Sustainable food packaging materials: Pollen g...  \n",
      "3    Biodegradable cosmetics: Pollen grains can be ...  \n",
      "4    Lightweight and strong materials: Pollen grain...  \n",
      "..                                                 ...  \n",
      "109  Plant-based vaccines and immunotherapies using...  \n",
      "110          Biodegradable textile fibers for clothing  \n",
      "111  Natural cosmetic and personal care products (e...  \n",
      "112  Sustainable agricultural inputs (e.g. Fertiliz...  \n",
      "113                 Eco-friendly and durable adhesives  \n",
      "\n",
      "[114 rows x 2 columns]\n",
      "The final list of ideas was generated and saved in divergentideas.csv.\n"
     ]
    }
   ],
   "source": [
    "from protocols.idea_div import sample_bullets, filter_unique_ideas\n",
    "\n",
    "finaldf = pd.DataFrame({\n",
    "    \"Prompt\":[],\n",
    "    \"Idea\":[],\n",
    "})\n",
    "\n",
    "while len(finaldf) < num_ideas:\n",
    "    gendf, bullets, bullcount = sample_bullets(1, num_per_gen, prompt,query_engine) \n",
    "    print(f\"{bullcount} ideas were generated\")\n",
    "    finaldf = filter_unique_ideas(finaldf, gendf, similarity_threshold= sim_thres)\n",
    "    print(f\"Unique ideas were added. Current number of ideas: {len(finaldf)}\")\n",
    "\n",
    "finaldf.to_csv(f'{csv_filename}.csv', index=False)\n",
    "print(finaldf)\n",
    "print(f\"The final list of ideas was generated and saved in {csv_filename}.csv.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa4cc5e",
   "metadata": {},
   "source": [
    "## Convergent Evaluation\n",
    "For the convergent evaluation phase, Llama-3.1-8b-instruct quantized to 8bit is used. Considering the memory of your system, you may need to restart the kernel and load only one model at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5007f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate, SimpleDirectoryReader, VectorStoreIndex, Settings\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "from typing import List, Optional, Sequence\n",
    "import pandas as pd\n",
    "from utils.formats import messages_to_prompt, completion_to_prompt\n",
    "\n",
    "url = \"https://huggingface.co/rachelkluu/Meta-Llama-3.1-8B-Instruct-Q8_0-GGUF/resolve/main/meta-llama-3.1-8b-instruct-q8_0.gguf\"\n",
    "llama31_q8 = LlamaCPP(\n",
    "    model_url=url,\n",
    "    model_path=None,\n",
    "    temperature=.1,\n",
    "    max_new_tokens=5000,\n",
    "    context_window=16000,\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "Settings.llm = llama31_q8 \n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"./RAG/\"   # FOLDER TO PAPERS OF INTEREST\n",
    ").load_data()\n",
    "\n",
    "Settings.chunk_size = 128\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = vector_index.as_query_engine(response_mode=\"compact\", similarity_top_k=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0919d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original prompt from divergent generation: \n",
    "prompt = \"What are other material applications that pollen grains can be used for?\"\n",
    "round_limit = 6 # Maximum number of elimination rounds to perform (for 100 ideas, 5-6 rounds)\n",
    "filename=[f\"divergentideas.csv\",] # csv file where ideas from divergent generation was saved \n",
    "filetagline = \"convergedideas\" # tagline for final outputted csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d417e92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Round 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.15 ms /    43 runs   (    0.10 ms per token, 10351.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1913.81 ms /  1485 tokens (    1.29 ms per token,   775.94 tokens per second)\n",
      "llama_print_timings:        eval time =    1382.03 ms /    42 runs   (   32.91 ms per token,    30.39 tokens per second)\n",
      "llama_print_timings:       total time =    3407.53 ms /  1527 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1129 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.01 ms /    12 runs   (    0.08 ms per token, 11846.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     738.25 ms /  1129 tokens (    0.65 ms per token,  1529.28 tokens per second)\n",
      "llama_print_timings:        eval time =     364.96 ms /    11 runs   (   33.18 ms per token,    30.14 tokens per second)\n",
      "llama_print_timings:       total time =    1130.35 ms /  1140 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1328 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.30 ms /    47 runs   (    0.09 ms per token, 10930.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     857.68 ms /  1328 tokens (    0.65 ms per token,  1548.36 tokens per second)\n",
      "llama_print_timings:        eval time =    1519.27 ms /    46 runs   (   33.03 ms per token,    30.28 tokens per second)\n",
      "llama_print_timings:       total time =    2508.15 ms /  1374 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1074 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.07 ms /    12 runs   (    0.09 ms per token, 11225.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     736.77 ms /  1074 tokens (    0.69 ms per token,  1457.71 tokens per second)\n",
      "llama_print_timings:        eval time =     354.93 ms /    11 runs   (   32.27 ms per token,    30.99 tokens per second)\n",
      "llama_print_timings:       total time =    1130.46 ms /  1085 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1292 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.07 ms /    43 runs   (    0.09 ms per token, 10570.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     842.15 ms /  1292 tokens (    0.65 ms per token,  1534.16 tokens per second)\n",
      "llama_print_timings:        eval time =    1360.07 ms /    42 runs   (   32.38 ms per token,    30.88 tokens per second)\n",
      "llama_print_timings:       total time =    2280.58 ms /  1334 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1258 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.18 ms /    12 runs   (    0.10 ms per token, 10178.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     822.77 ms /  1258 tokens (    0.65 ms per token,  1528.98 tokens per second)\n",
      "llama_print_timings:        eval time =     355.06 ms /    11 runs   (   32.28 ms per token,    30.98 tokens per second)\n",
      "llama_print_timings:       total time =    1199.71 ms /  1269 tokens\n",
      "Llama.generate: 355 prefix-match hit, remaining 1046 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.90 ms /    47 runs   (    0.10 ms per token,  9595.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     707.57 ms /  1046 tokens (    0.68 ms per token,  1478.30 tokens per second)\n",
      "llama_print_timings:        eval time =    1494.04 ms /    46 runs   (   32.48 ms per token,    30.79 tokens per second)\n",
      "llama_print_timings:       total time =    2284.16 ms /  1092 tokens\n",
      "Llama.generate: 464 prefix-match hit, remaining 871 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.28 ms /    12 runs   (    0.11 ms per token,  9397.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     599.41 ms /   871 tokens (    0.69 ms per token,  1453.09 tokens per second)\n",
      "llama_print_timings:        eval time =     360.17 ms /    11 runs   (   32.74 ms per token,    30.54 tokens per second)\n",
      "llama_print_timings:       total time =     977.44 ms /   882 tokens\n",
      "Llama.generate: 401 prefix-match hit, remaining 962 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.50 ms /    47 runs   (    0.10 ms per token, 10453.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     642.76 ms /   962 tokens (    0.67 ms per token,  1496.67 tokens per second)\n",
      "llama_print_timings:        eval time =    1493.88 ms /    46 runs   (   32.48 ms per token,    30.79 tokens per second)\n",
      "llama_print_timings:       total time =    2193.01 ms /  1008 tokens\n",
      "Llama.generate: 136 prefix-match hit, remaining 1193 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.05 ms /    12 runs   (    0.09 ms per token, 11450.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     800.76 ms /  1193 tokens (    0.67 ms per token,  1489.83 tokens per second)\n",
      "llama_print_timings:        eval time =     374.41 ms /    11 runs   (   34.04 ms per token,    29.38 tokens per second)\n",
      "llama_print_timings:       total time =    1196.94 ms /  1204 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1340 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.10 ms /    47 runs   (    0.09 ms per token, 11474.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =     868.37 ms /  1340 tokens (    0.65 ms per token,  1543.12 tokens per second)\n",
      "llama_print_timings:        eval time =    1492.91 ms /    46 runs   (   32.45 ms per token,    30.81 tokens per second)\n",
      "llama_print_timings:       total time =    2416.16 ms /  1386 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1274 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.07 ms /    12 runs   (    0.09 ms per token, 11173.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     845.76 ms /  1274 tokens (    0.66 ms per token,  1506.34 tokens per second)\n",
      "llama_print_timings:        eval time =     369.21 ms /    11 runs   (   33.56 ms per token,    29.79 tokens per second)\n",
      "llama_print_timings:       total time =    1230.80 ms /  1285 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1100 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       5.02 ms /    47 runs   (    0.11 ms per token,  9364.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     727.14 ms /  1100 tokens (    0.66 ms per token,  1512.78 tokens per second)\n",
      "llama_print_timings:        eval time =    1522.58 ms /    46 runs   (   33.10 ms per token,    30.21 tokens per second)\n",
      "llama_print_timings:       total time =    2311.87 ms /  1146 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1059 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.45 ms /    12 runs   (    0.12 ms per token,  8281.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     734.98 ms /  1059 tokens (    0.69 ms per token,  1440.86 tokens per second)\n",
      "llama_print_timings:        eval time =     373.01 ms /    11 runs   (   33.91 ms per token,    29.49 tokens per second)\n",
      "llama_print_timings:       total time =    1127.74 ms /  1070 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1300 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.30 ms /    47 runs   (    0.09 ms per token, 10940.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     860.48 ms /  1300 tokens (    0.66 ms per token,  1510.78 tokens per second)\n",
      "llama_print_timings:        eval time =    1502.41 ms /    46 runs   (   32.66 ms per token,    30.62 tokens per second)\n",
      "llama_print_timings:       total time =    2418.86 ms /  1346 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1063 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.51 ms /    12 runs   (    0.13 ms per token,  7936.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     730.57 ms /  1063 tokens (    0.69 ms per token,  1455.02 tokens per second)\n",
      "llama_print_timings:        eval time =     364.58 ms /    11 runs   (   33.14 ms per token,    30.17 tokens per second)\n",
      "llama_print_timings:       total time =    1112.85 ms /  1074 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1336 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.53 ms /    47 runs   (    0.10 ms per token, 10382.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     867.00 ms /  1336 tokens (    0.65 ms per token,  1540.94 tokens per second)\n",
      "llama_print_timings:        eval time =    1473.46 ms /    46 runs   (   32.03 ms per token,    31.22 tokens per second)\n",
      "llama_print_timings:       total time =    2396.90 ms /  1382 tokens\n",
      "Llama.generate: 136 prefix-match hit, remaining 1179 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.96 ms /    12 runs   (    0.08 ms per token, 12474.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     769.96 ms /  1179 tokens (    0.65 ms per token,  1531.26 tokens per second)\n",
      "llama_print_timings:        eval time =     352.37 ms /    11 runs   (   32.03 ms per token,    31.22 tokens per second)\n",
      "llama_print_timings:       total time =    1136.34 ms /  1190 tokens\n",
      "Llama.generate: 136 prefix-match hit, remaining 1253 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.64 ms /    47 runs   (    0.10 ms per token, 10124.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     814.49 ms /  1253 tokens (    0.65 ms per token,  1538.38 tokens per second)\n",
      "llama_print_timings:        eval time =    1498.61 ms /    46 runs   (   32.58 ms per token,    30.70 tokens per second)\n",
      "llama_print_timings:       total time =    2372.93 ms /  1299 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1079 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.25 ms /    12 runs   (    0.10 ms per token,  9607.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     739.88 ms /  1079 tokens (    0.69 ms per token,  1458.35 tokens per second)\n",
      "llama_print_timings:        eval time =     352.88 ms /    11 runs   (   32.08 ms per token,    31.17 tokens per second)\n",
      "llama_print_timings:       total time =    1107.20 ms /  1090 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1359 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.50 ms /    47 runs   (    0.10 ms per token, 10449.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     850.36 ms /  1359 tokens (    0.63 ms per token,  1598.15 tokens per second)\n",
      "llama_print_timings:        eval time =    1496.66 ms /    46 runs   (   32.54 ms per token,    30.74 tokens per second)\n",
      "llama_print_timings:       total time =    2407.19 ms /  1405 tokens\n",
      "Llama.generate: 366 prefix-match hit, remaining 992 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.72 ms /    12 runs   (    0.14 ms per token,  6964.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     675.57 ms /   992 tokens (    0.68 ms per token,  1468.39 tokens per second)\n",
      "llama_print_timings:        eval time =     354.38 ms /    11 runs   (   32.22 ms per token,    31.04 tokens per second)\n",
      "llama_print_timings:       total time =    1049.00 ms /  1003 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1272 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.62 ms /    47 runs   (    0.10 ms per token, 10164.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     800.07 ms /  1272 tokens (    0.63 ms per token,  1589.86 tokens per second)\n",
      "llama_print_timings:        eval time =    1491.17 ms /    46 runs   (   32.42 ms per token,    30.85 tokens per second)\n",
      "llama_print_timings:       total time =    2346.16 ms /  1318 tokens\n",
      "Llama.generate: 136 prefix-match hit, remaining 1195 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.60 ms /    12 runs   (    0.13 ms per token,  7500.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     787.51 ms /  1195 tokens (    0.66 ms per token,  1517.44 tokens per second)\n",
      "llama_print_timings:        eval time =     363.57 ms /    11 runs   (   33.05 ms per token,    30.26 tokens per second)\n",
      "llama_print_timings:       total time =    1168.25 ms /  1206 tokens\n",
      "Llama.generate: 136 prefix-match hit, remaining 1265 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       5.36 ms /    47 runs   (    0.11 ms per token,  8763.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     826.07 ms /  1265 tokens (    0.65 ms per token,  1531.34 tokens per second)\n",
      "llama_print_timings:        eval time =    1503.92 ms /    46 runs   (   32.69 ms per token,    30.59 tokens per second)\n",
      "llama_print_timings:       total time =    2391.74 ms /  1311 tokens\n",
      "Llama.generate: 136 prefix-match hit, remaining 1255 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.78 ms /    12 runs   (    0.15 ms per token,  6756.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     840.60 ms /  1255 tokens (    0.67 ms per token,  1492.99 tokens per second)\n",
      "llama_print_timings:        eval time =     362.98 ms /    11 runs   (   33.00 ms per token,    30.30 tokens per second)\n",
      "llama_print_timings:       total time =    1221.37 ms /  1266 tokens\n",
      "Llama.generate: 136 prefix-match hit, remaining 1226 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       5.63 ms /    47 runs   (    0.12 ms per token,  8345.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     781.88 ms /  1226 tokens (    0.64 ms per token,  1568.01 tokens per second)\n",
      "llama_print_timings:        eval time =    1506.78 ms /    46 runs   (   32.76 ms per token,    30.53 tokens per second)\n",
      "llama_print_timings:       total time =    2345.72 ms /  1272 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1065 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.97 ms /    12 runs   (    0.08 ms per token, 12307.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     720.31 ms /  1065 tokens (    0.68 ms per token,  1478.53 tokens per second)\n",
      "llama_print_timings:        eval time =     360.25 ms /    11 runs   (   32.75 ms per token,    30.53 tokens per second)\n",
      "llama_print_timings:       total time =    1092.94 ms /  1076 tokens\n",
      "Llama.generate: 182 prefix-match hit, remaining 1202 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       5.80 ms /    47 runs   (    0.12 ms per token,  8097.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     774.08 ms /  1202 tokens (    0.64 ms per token,  1552.82 tokens per second)\n",
      "llama_print_timings:        eval time =    1513.09 ms /    46 runs   (   32.89 ms per token,    30.40 tokens per second)\n",
      "llama_print_timings:       total time =    2347.44 ms /  1248 tokens\n",
      "Llama.generate: 252 prefix-match hit, remaining 1087 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.11 ms /    12 runs   (    0.09 ms per token, 10840.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     734.20 ms /  1087 tokens (    0.68 ms per token,  1480.52 tokens per second)\n",
      "llama_print_timings:        eval time =     361.48 ms /    11 runs   (   32.86 ms per token,    30.43 tokens per second)\n",
      "llama_print_timings:       total time =    1110.43 ms /  1098 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1408 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       6.62 ms /    47 runs   (    0.14 ms per token,  7096.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     883.08 ms /  1408 tokens (    0.63 ms per token,  1594.42 tokens per second)\n",
      "llama_print_timings:        eval time =    1523.73 ms /    46 runs   (   33.12 ms per token,    30.19 tokens per second)\n",
      "llama_print_timings:       total time =    2469.77 ms /  1454 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1255 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.29 ms /    12 runs   (    0.11 ms per token,  9316.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     821.69 ms /  1255 tokens (    0.65 ms per token,  1527.34 tokens per second)\n",
      "llama_print_timings:        eval time =     371.75 ms /    11 runs   (   33.80 ms per token,    29.59 tokens per second)\n",
      "llama_print_timings:       total time =    1207.80 ms /  1266 tokens\n",
      "Llama.generate: 182 prefix-match hit, remaining 1201 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       5.78 ms /    47 runs   (    0.12 ms per token,  8130.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     778.05 ms /  1201 tokens (    0.65 ms per token,  1543.61 tokens per second)\n",
      "llama_print_timings:        eval time =    1515.51 ms /    46 runs   (   32.95 ms per token,    30.35 tokens per second)\n",
      "llama_print_timings:       total time =    2351.60 ms /  1247 tokens\n",
      "Llama.generate: 182 prefix-match hit, remaining 1148 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.37 ms /    12 runs   (    0.11 ms per token,  8746.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     753.52 ms /  1148 tokens (    0.66 ms per token,  1523.51 tokens per second)\n",
      "llama_print_timings:        eval time =     357.27 ms /    11 runs   (   32.48 ms per token,    30.79 tokens per second)\n",
      "llama_print_timings:       total time =    1126.24 ms /  1159 tokens\n",
      "Llama.generate: 182 prefix-match hit, remaining 1186 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       5.99 ms /    47 runs   (    0.13 ms per token,  7842.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     801.21 ms /  1186 tokens (    0.68 ms per token,  1480.27 tokens per second)\n",
      "llama_print_timings:        eval time =    1554.96 ms /    46 runs   (   33.80 ms per token,    29.58 tokens per second)\n",
      "llama_print_timings:       total time =    2421.42 ms /  1232 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 1298 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.17 ms /    12 runs   (    0.10 ms per token, 10291.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     855.32 ms /  1298 tokens (    0.66 ms per token,  1517.57 tokens per second)\n",
      "llama_print_timings:        eval time =     379.12 ms /    11 runs   (   34.47 ms per token,    29.01 tokens per second)\n",
      "llama_print_timings:       total time =    1251.36 ms /  1309 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 1325 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       5.55 ms /    47 runs   (    0.12 ms per token,  8471.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     859.44 ms /  1325 tokens (    0.65 ms per token,  1541.71 tokens per second)\n",
      "llama_print_timings:        eval time =    1520.36 ms /    46 runs   (   33.05 ms per token,    30.26 tokens per second)\n",
      "llama_print_timings:       total time =    2443.13 ms /  1371 tokens\n",
      "Llama.generate: 136 prefix-match hit, remaining 1184 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.38 ms /    12 runs   (    0.11 ms per token,  8727.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     784.20 ms /  1184 tokens (    0.66 ms per token,  1509.82 tokens per second)\n",
      "llama_print_timings:        eval time =     378.04 ms /    11 runs   (   34.37 ms per token,    29.10 tokens per second)\n",
      "llama_print_timings:       total time =    1179.71 ms /  1195 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1308 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       5.87 ms /    47 runs   (    0.12 ms per token,  8009.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     866.70 ms /  1308 tokens (    0.66 ms per token,  1509.18 tokens per second)\n",
      "llama_print_timings:        eval time =    1543.12 ms /    46 runs   (   33.55 ms per token,    29.81 tokens per second)\n",
      "llama_print_timings:       total time =    2477.53 ms /  1354 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1269 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.13 ms /    12 runs   (    0.09 ms per token, 10610.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     843.45 ms /  1269 tokens (    0.66 ms per token,  1504.53 tokens per second)\n",
      "llama_print_timings:        eval time =     362.41 ms /    11 runs   (   32.95 ms per token,    30.35 tokens per second)\n",
      "llama_print_timings:       total time =    1220.91 ms /  1280 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1127 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.65 ms /    47 runs   (    0.10 ms per token, 10116.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     728.41 ms /  1127 tokens (    0.65 ms per token,  1547.21 tokens per second)\n",
      "llama_print_timings:        eval time =    1532.34 ms /    46 runs   (   33.31 ms per token,    30.02 tokens per second)\n",
      "llama_print_timings:       total time =    2321.14 ms /  1173 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1066 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.42 ms /    12 runs   (    0.12 ms per token,  8426.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     758.70 ms /  1066 tokens (    0.71 ms per token,  1405.03 tokens per second)\n",
      "llama_print_timings:        eval time =     368.52 ms /    11 runs   (   33.50 ms per token,    29.85 tokens per second)\n",
      "llama_print_timings:       total time =    1143.70 ms /  1077 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1101 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       6.01 ms /    47 runs   (    0.13 ms per token,  7824.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     719.50 ms /  1101 tokens (    0.65 ms per token,  1530.24 tokens per second)\n",
      "llama_print_timings:        eval time =    1499.63 ms /    46 runs   (   32.60 ms per token,    30.67 tokens per second)\n",
      "llama_print_timings:       total time =    2280.64 ms /  1147 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1082 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.41 ms /    12 runs   (    0.12 ms per token,  8540.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     756.96 ms /  1082 tokens (    0.70 ms per token,  1429.40 tokens per second)\n",
      "llama_print_timings:        eval time =     370.05 ms /    11 runs   (   33.64 ms per token,    29.73 tokens per second)\n",
      "llama_print_timings:       total time =    1144.21 ms /  1093 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1333 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.92 ms /    47 runs   (    0.10 ms per token,  9558.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     873.20 ms /  1333 tokens (    0.66 ms per token,  1526.57 tokens per second)\n",
      "llama_print_timings:        eval time =    1500.16 ms /    46 runs   (   32.61 ms per token,    30.66 tokens per second)\n",
      "llama_print_timings:       total time =    2430.14 ms /  1379 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1088 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.96 ms /    12 runs   (    0.08 ms per token, 12552.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     700.27 ms /  1088 tokens (    0.64 ms per token,  1553.69 tokens per second)\n",
      "llama_print_timings:        eval time =     350.39 ms /    11 runs   (   31.85 ms per token,    31.39 tokens per second)\n",
      "llama_print_timings:       total time =    1063.08 ms /  1099 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1377 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.73 ms /    43 runs   (    0.11 ms per token,  9098.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     886.26 ms /  1377 tokens (    0.64 ms per token,  1553.71 tokens per second)\n",
      "llama_print_timings:        eval time =    1365.08 ms /    42 runs   (   32.50 ms per token,    30.77 tokens per second)\n",
      "llama_print_timings:       total time =    2304.70 ms /  1419 tokens\n",
      "Llama.generate: 528 prefix-match hit, remaining 836 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.92 ms /    12 runs   (    0.08 ms per token, 13001.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     553.72 ms /   836 tokens (    0.66 ms per token,  1509.79 tokens per second)\n",
      "llama_print_timings:        eval time =     359.46 ms /    11 runs   (   32.68 ms per token,    30.60 tokens per second)\n",
      "llama_print_timings:       total time =     926.35 ms /   847 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1318 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.48 ms /    43 runs   (    0.10 ms per token,  9591.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     851.79 ms /  1318 tokens (    0.65 ms per token,  1547.33 tokens per second)\n",
      "llama_print_timings:        eval time =    1372.11 ms /    42 runs   (   32.67 ms per token,    30.61 tokens per second)\n",
      "llama_print_timings:       total time =    2274.49 ms /  1360 tokens\n",
      "Llama.generate: 359 prefix-match hit, remaining 1012 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.36 ms /    12 runs   (    0.11 ms per token,  8817.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     676.28 ms /  1012 tokens (    0.67 ms per token,  1496.43 tokens per second)\n",
      "llama_print_timings:        eval time =     359.76 ms /    11 runs   (   32.71 ms per token,    30.58 tokens per second)\n",
      "llama_print_timings:       total time =    1051.42 ms /  1023 tokens\n",
      "Llama.generate: 296 prefix-match hit, remaining 1077 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       5.23 ms /    47 runs   (    0.11 ms per token,  8988.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     725.07 ms /  1077 tokens (    0.67 ms per token,  1485.37 tokens per second)\n",
      "llama_print_timings:        eval time =    1524.81 ms /    46 runs   (   33.15 ms per token,    30.17 tokens per second)\n",
      "llama_print_timings:       total time =    2308.79 ms /  1123 tokens\n",
      "Llama.generate: 296 prefix-match hit, remaining 1020 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.13 ms /    12 runs   (    0.09 ms per token, 10610.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     701.24 ms /  1020 tokens (    0.69 ms per token,  1454.57 tokens per second)\n",
      "llama_print_timings:        eval time =     365.55 ms /    11 runs   (   33.23 ms per token,    30.09 tokens per second)\n",
      "llama_print_timings:       total time =    1080.14 ms /  1031 tokens\n",
      "Llama.generate: 296 prefix-match hit, remaining 1088 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       6.35 ms /    47 runs   (    0.14 ms per token,  7406.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     737.95 ms /  1088 tokens (    0.68 ms per token,  1474.36 tokens per second)\n",
      "llama_print_timings:        eval time =    1543.36 ms /    46 runs   (   33.55 ms per token,    29.81 tokens per second)\n",
      "llama_print_timings:       total time =    2348.47 ms /  1134 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1085 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.93 ms /    12 runs   (    0.16 ms per token,  6214.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     750.80 ms /  1085 tokens (    0.69 ms per token,  1445.12 tokens per second)\n",
      "llama_print_timings:        eval time =     372.15 ms /    11 runs   (   33.83 ms per token,    29.56 tokens per second)\n",
      "llama_print_timings:       total time =    1141.99 ms /  1096 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1339 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       6.58 ms /    47 runs   (    0.14 ms per token,  7143.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     890.35 ms /  1339 tokens (    0.66 ms per token,  1503.91 tokens per second)\n",
      "llama_print_timings:        eval time =    1553.43 ms /    46 runs   (   33.77 ms per token,    29.61 tokens per second)\n",
      "llama_print_timings:       total time =    2512.00 ms /  1385 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1101 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.28 ms /    12 runs   (    0.11 ms per token,  9411.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     762.72 ms /  1101 tokens (    0.69 ms per token,  1443.52 tokens per second)\n",
      "llama_print_timings:        eval time =     370.15 ms /    11 runs   (   33.65 ms per token,    29.72 tokens per second)\n",
      "llama_print_timings:       total time =    1148.18 ms /  1112 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1149 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       5.09 ms /    47 runs   (    0.11 ms per token,  9226.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     743.97 ms /  1149 tokens (    0.65 ms per token,  1544.41 tokens per second)\n",
      "llama_print_timings:        eval time =    1528.59 ms /    46 runs   (   33.23 ms per token,    30.09 tokens per second)\n",
      "llama_print_timings:       total time =    2329.25 ms /  1195 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1280 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.06 ms /    12 runs   (    0.09 ms per token, 11278.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     837.01 ms /  1280 tokens (    0.65 ms per token,  1529.25 tokens per second)\n",
      "llama_print_timings:        eval time =     367.63 ms /    11 runs   (   33.42 ms per token,    29.92 tokens per second)\n",
      "llama_print_timings:       total time =    1217.91 ms /  1291 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1101 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       6.33 ms /    47 runs   (    0.13 ms per token,  7419.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     735.09 ms /  1101 tokens (    0.67 ms per token,  1497.77 tokens per second)\n",
      "llama_print_timings:        eval time =    1525.11 ms /    46 runs   (   33.15 ms per token,    30.16 tokens per second)\n",
      "llama_print_timings:       total time =    2325.28 ms /  1147 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1078 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.33 ms /    12 runs   (    0.11 ms per token,  9049.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     747.56 ms /  1078 tokens (    0.69 ms per token,  1442.02 tokens per second)\n",
      "llama_print_timings:        eval time =     361.84 ms /    11 runs   (   32.89 ms per token,    30.40 tokens per second)\n",
      "llama_print_timings:       total time =    1125.58 ms /  1089 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1286 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.71 ms /    47 runs   (    0.10 ms per token,  9985.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     843.48 ms /  1286 tokens (    0.66 ms per token,  1524.64 tokens per second)\n",
      "llama_print_timings:        eval time =    1514.41 ms /    46 runs   (   32.92 ms per token,    30.37 tokens per second)\n",
      "llama_print_timings:       total time =    2412.80 ms /  1332 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1245 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.06 ms /    12 runs   (    0.09 ms per token, 11320.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     786.81 ms /  1245 tokens (    0.63 ms per token,  1582.34 tokens per second)\n",
      "llama_print_timings:        eval time =     377.50 ms /    11 runs   (   34.32 ms per token,    29.14 tokens per second)\n",
      "llama_print_timings:       total time =    1178.65 ms /  1256 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1157 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       5.13 ms /    47 runs   (    0.11 ms per token,  9161.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     778.13 ms /  1157 tokens (    0.67 ms per token,  1486.89 tokens per second)\n",
      "llama_print_timings:        eval time =    1514.05 ms /    46 runs   (   32.91 ms per token,    30.38 tokens per second)\n",
      "llama_print_timings:       total time =    2350.47 ms /  1203 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1066 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.23 ms /    12 runs   (    0.10 ms per token,  9740.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     761.10 ms /  1066 tokens (    0.71 ms per token,  1400.60 tokens per second)\n",
      "llama_print_timings:        eval time =     354.60 ms /    11 runs   (   32.24 ms per token,    31.02 tokens per second)\n",
      "llama_print_timings:       total time =    1132.42 ms /  1077 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1336 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       5.15 ms /    47 runs   (    0.11 ms per token,  9129.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     888.60 ms /  1336 tokens (    0.67 ms per token,  1503.49 tokens per second)\n",
      "llama_print_timings:        eval time =    1541.12 ms /    46 runs   (   33.50 ms per token,    29.85 tokens per second)\n",
      "llama_print_timings:       total time =    2490.45 ms /  1382 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 1305 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.94 ms /    12 runs   (    0.08 ms per token, 12725.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     869.40 ms /  1305 tokens (    0.67 ms per token,  1501.03 tokens per second)\n",
      "llama_print_timings:        eval time =     370.21 ms /    11 runs   (   33.66 ms per token,    29.71 tokens per second)\n",
      "llama_print_timings:       total time =    1253.16 ms /  1316 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 1333 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.64 ms /    47 runs   (    0.10 ms per token, 10131.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =     883.09 ms /  1333 tokens (    0.66 ms per token,  1509.48 tokens per second)\n",
      "llama_print_timings:        eval time =    1521.91 ms /    46 runs   (   33.09 ms per token,    30.23 tokens per second)\n",
      "llama_print_timings:       total time =    2461.58 ms /  1379 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1067 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.98 ms /    12 runs   (    0.08 ms per token, 12295.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     753.88 ms /  1067 tokens (    0.71 ms per token,  1415.34 tokens per second)\n",
      "llama_print_timings:        eval time =     365.99 ms /    11 runs   (   33.27 ms per token,    30.06 tokens per second)\n",
      "llama_print_timings:       total time =    1133.98 ms /  1078 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1097 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       5.23 ms /    43 runs   (    0.12 ms per token,  8226.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     736.96 ms /  1097 tokens (    0.67 ms per token,  1488.56 tokens per second)\n",
      "llama_print_timings:        eval time =    1390.01 ms /    42 runs   (   33.10 ms per token,    30.22 tokens per second)\n",
      "llama_print_timings:       total time =    2185.92 ms /  1139 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1092 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.90 ms /    12 runs   (    0.07 ms per token, 13333.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     717.61 ms /  1092 tokens (    0.66 ms per token,  1521.71 tokens per second)\n",
      "llama_print_timings:        eval time =     363.99 ms /    11 runs   (   33.09 ms per token,    30.22 tokens per second)\n",
      "llama_print_timings:       total time =    1094.57 ms /  1103 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1094 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.32 ms /    47 runs   (    0.09 ms per token, 10882.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     710.07 ms /  1094 tokens (    0.65 ms per token,  1540.70 tokens per second)\n",
      "llama_print_timings:        eval time =    1507.10 ms /    46 runs   (   32.76 ms per token,    30.52 tokens per second)\n",
      "llama_print_timings:       total time =    2269.97 ms /  1140 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1070 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.19 ms /    12 runs   (    0.10 ms per token, 10118.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     739.87 ms /  1070 tokens (    0.69 ms per token,  1446.20 tokens per second)\n",
      "llama_print_timings:        eval time =     368.03 ms /    11 runs   (   33.46 ms per token,    29.89 tokens per second)\n",
      "llama_print_timings:       total time =    1122.50 ms /  1081 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1342 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.29 ms /    43 runs   (    0.10 ms per token, 10018.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     880.80 ms /  1342 tokens (    0.66 ms per token,  1523.61 tokens per second)\n",
      "llama_print_timings:        eval time =    1390.29 ms /    42 runs   (   33.10 ms per token,    30.21 tokens per second)\n",
      "llama_print_timings:       total time =    2323.14 ms /  1384 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1312 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.12 ms /    12 runs   (    0.09 ms per token, 10685.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     852.82 ms /  1312 tokens (    0.65 ms per token,  1538.43 tokens per second)\n",
      "llama_print_timings:        eval time =     368.50 ms /    11 runs   (   33.50 ms per token,    29.85 tokens per second)\n",
      "llama_print_timings:       total time =    1236.21 ms /  1323 tokens\n",
      "Llama.generate: 355 prefix-match hit, remaining 1036 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       7.27 ms /    47 runs   (    0.15 ms per token,  6468.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     765.99 ms /  1036 tokens (    0.74 ms per token,  1352.50 tokens per second)\n",
      "llama_print_timings:        eval time =    1542.09 ms /    46 runs   (   33.52 ms per token,    29.83 tokens per second)\n",
      "llama_print_timings:       total time =    2385.89 ms /  1082 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 1315 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.49 ms /    12 runs   (    0.12 ms per token,  8048.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     891.38 ms /  1315 tokens (    0.68 ms per token,  1475.24 tokens per second)\n",
      "llama_print_timings:        eval time =     370.21 ms /    11 runs   (   33.66 ms per token,    29.71 tokens per second)\n",
      "llama_print_timings:       total time =    1276.23 ms /  1326 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 1373 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       5.74 ms /    47 runs   (    0.12 ms per token,  8182.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     875.46 ms /  1373 tokens (    0.64 ms per token,  1568.32 tokens per second)\n",
      "llama_print_timings:        eval time =    1507.74 ms /    46 runs   (   32.78 ms per token,    30.51 tokens per second)\n",
      "llama_print_timings:       total time =    2441.01 ms /  1419 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1291 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       2.03 ms /    12 runs   (    0.17 ms per token,  5923.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     872.01 ms /  1291 tokens (    0.68 ms per token,  1480.49 tokens per second)\n",
      "llama_print_timings:        eval time =     383.85 ms /    11 runs   (   34.90 ms per token,    28.66 tokens per second)\n",
      "llama_print_timings:       total time =    1276.46 ms /  1302 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1316 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       6.38 ms /    47 runs   (    0.14 ms per token,  7371.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     860.65 ms /  1316 tokens (    0.65 ms per token,  1529.07 tokens per second)\n",
      "llama_print_timings:        eval time =    1512.40 ms /    46 runs   (   32.88 ms per token,    30.42 tokens per second)\n",
      "llama_print_timings:       total time =    2437.47 ms /  1362 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1251 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       2.27 ms /    12 runs   (    0.19 ms per token,  5293.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     802.23 ms /  1251 tokens (    0.64 ms per token,  1559.40 tokens per second)\n",
      "llama_print_timings:        eval time =     368.79 ms /    11 runs   (   33.53 ms per token,    29.83 tokens per second)\n",
      "llama_print_timings:       total time =    1192.10 ms /  1262 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1294 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.31 ms /    47 runs   (    0.09 ms per token, 10907.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     856.41 ms /  1294 tokens (    0.66 ms per token,  1510.96 tokens per second)\n",
      "llama_print_timings:        eval time =    1492.53 ms /    46 runs   (   32.45 ms per token,    30.82 tokens per second)\n",
      "llama_print_timings:       total time =    2407.26 ms /  1340 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1254 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.08 ms /    12 runs   (    0.09 ms per token, 11111.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     831.13 ms /  1254 tokens (    0.66 ms per token,  1508.79 tokens per second)\n",
      "llama_print_timings:        eval time =     355.39 ms /    11 runs   (   32.31 ms per token,    30.95 tokens per second)\n",
      "llama_print_timings:       total time =    1202.11 ms /  1265 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1315 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       5.35 ms /    47 runs   (    0.11 ms per token,  8786.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     865.23 ms /  1315 tokens (    0.66 ms per token,  1519.82 tokens per second)\n",
      "llama_print_timings:        eval time =    1520.06 ms /    46 runs   (   33.04 ms per token,    30.26 tokens per second)\n",
      "llama_print_timings:       total time =    2443.31 ms /  1361 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1266 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.19 ms /    12 runs   (    0.10 ms per token, 10126.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     824.49 ms /  1266 tokens (    0.65 ms per token,  1535.49 tokens per second)\n",
      "llama_print_timings:        eval time =     368.93 ms /    11 runs   (   33.54 ms per token,    29.82 tokens per second)\n",
      "llama_print_timings:       total time =    1207.76 ms /  1277 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1375 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       5.43 ms /    47 runs   (    0.12 ms per token,  8658.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     900.51 ms /  1375 tokens (    0.65 ms per token,  1526.91 tokens per second)\n",
      "llama_print_timings:        eval time =    1513.36 ms /    46 runs   (   32.90 ms per token,    30.40 tokens per second)\n",
      "llama_print_timings:       total time =    2473.13 ms /  1421 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1356 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.70 ms /    12 runs   (    0.14 ms per token,  7046.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     874.67 ms /  1356 tokens (    0.65 ms per token,  1550.31 tokens per second)\n",
      "llama_print_timings:        eval time =     367.49 ms /    11 runs   (   33.41 ms per token,    29.93 tokens per second)\n",
      "llama_print_timings:       total time =    1259.31 ms /  1367 tokens\n",
      "Llama.generate: 182 prefix-match hit, remaining 1182 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       5.79 ms /    47 runs   (    0.12 ms per token,  8123.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     793.12 ms /  1182 tokens (    0.67 ms per token,  1490.31 tokens per second)\n",
      "llama_print_timings:        eval time =    1536.36 ms /    46 runs   (   33.40 ms per token,    29.94 tokens per second)\n",
      "llama_print_timings:       total time =    2390.83 ms /  1228 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1074 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /    12 runs   (    0.10 ms per token, 10407.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     743.56 ms /  1074 tokens (    0.69 ms per token,  1444.41 tokens per second)\n",
      "llama_print_timings:        eval time =     360.69 ms /    11 runs   (   32.79 ms per token,    30.50 tokens per second)\n",
      "llama_print_timings:       total time =    1118.46 ms /  1085 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1066 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       7.26 ms /    47 runs   (    0.15 ms per token,  6472.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     698.07 ms /  1066 tokens (    0.65 ms per token,  1527.08 tokens per second)\n",
      "llama_print_timings:        eval time =    1527.54 ms /    46 runs   (   33.21 ms per token,    30.11 tokens per second)\n",
      "llama_print_timings:       total time =    2295.71 ms /  1112 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1046 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.70 ms /    12 runs   (    0.14 ms per token,  7067.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     744.97 ms /  1046 tokens (    0.71 ms per token,  1404.09 tokens per second)\n",
      "llama_print_timings:        eval time =     369.23 ms /    11 runs   (   33.57 ms per token,    29.79 tokens per second)\n",
      "llama_print_timings:       total time =    1136.35 ms /  1057 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1308 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       5.76 ms /    47 runs   (    0.12 ms per token,  8165.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     852.98 ms /  1308 tokens (    0.65 ms per token,  1533.45 tokens per second)\n",
      "llama_print_timings:        eval time =    1524.70 ms /    46 runs   (   33.15 ms per token,    30.17 tokens per second)\n",
      "llama_print_timings:       total time =    2438.74 ms /  1354 tokens\n",
      "Llama.generate: 139 prefix-match hit, remaining 1209 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.97 ms /    12 runs   (    0.08 ms per token, 12396.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     834.38 ms /  1209 tokens (    0.69 ms per token,  1448.98 tokens per second)\n",
      "llama_print_timings:        eval time =     364.02 ms /    11 runs   (   33.09 ms per token,    30.22 tokens per second)\n",
      "llama_print_timings:       total time =    1212.94 ms /  1220 tokens\n",
      "Llama.generate: 139 prefix-match hit, remaining 1215 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.74 ms /    47 runs   (    0.10 ms per token,  9923.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     805.38 ms /  1215 tokens (    0.66 ms per token,  1508.60 tokens per second)\n",
      "llama_print_timings:        eval time =    1521.55 ms /    46 runs   (   33.08 ms per token,    30.23 tokens per second)\n",
      "llama_print_timings:       total time =    2382.93 ms /  1261 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1248 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.02 ms /    12 runs   (    0.08 ms per token, 11811.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     840.97 ms /  1248 tokens (    0.67 ms per token,  1484.01 tokens per second)\n",
      "llama_print_timings:        eval time =     368.95 ms /    11 runs   (   33.54 ms per token,    29.81 tokens per second)\n",
      "llama_print_timings:       total time =    1223.57 ms /  1259 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1152 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       7.04 ms /    47 runs   (    0.15 ms per token,  6673.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =     775.94 ms /  1152 tokens (    0.67 ms per token,  1484.66 tokens per second)\n",
      "llama_print_timings:        eval time =    1587.00 ms /    46 runs   (   34.50 ms per token,    28.99 tokens per second)\n",
      "llama_print_timings:       total time =    2435.24 ms /  1198 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1099 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.95 ms /    12 runs   (    0.08 ms per token, 12631.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     772.49 ms /  1099 tokens (    0.70 ms per token,  1422.68 tokens per second)\n",
      "llama_print_timings:        eval time =     366.91 ms /    11 runs   (   33.36 ms per token,    29.98 tokens per second)\n",
      "llama_print_timings:       total time =    1152.98 ms /  1110 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1179 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.65 ms /    43 runs   (    0.11 ms per token,  9243.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     793.68 ms /  1179 tokens (    0.67 ms per token,  1485.49 tokens per second)\n",
      "llama_print_timings:        eval time =    1401.37 ms /    42 runs   (   33.37 ms per token,    29.97 tokens per second)\n",
      "llama_print_timings:       total time =    2248.41 ms /  1221 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1081 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.95 ms /    12 runs   (    0.08 ms per token, 12591.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     708.82 ms /  1081 tokens (    0.66 ms per token,  1525.07 tokens per second)\n",
      "llama_print_timings:        eval time =     348.59 ms /    11 runs   (   31.69 ms per token,    31.56 tokens per second)\n",
      "llama_print_timings:       total time =    1069.99 ms /  1092 tokens\n",
      "Llama.generate: 464 prefix-match hit, remaining 940 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       7.64 ms /    47 runs   (    0.16 ms per token,  6155.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     635.32 ms /   940 tokens (    0.68 ms per token,  1479.56 tokens per second)\n",
      "llama_print_timings:        eval time =    1552.81 ms /    46 runs   (   33.76 ms per token,    29.62 tokens per second)\n",
      "llama_print_timings:       total time =    2265.39 ms /   986 tokens\n",
      "Llama.generate: 136 prefix-match hit, remaining 1218 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.06 ms /    12 runs   (    0.09 ms per token, 11342.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     812.12 ms /  1218 tokens (    0.67 ms per token,  1499.78 tokens per second)\n",
      "llama_print_timings:        eval time =     377.09 ms /    11 runs   (   34.28 ms per token,    29.17 tokens per second)\n",
      "llama_print_timings:       total time =    1203.64 ms /  1229 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1281 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       6.26 ms /    47 runs   (    0.13 ms per token,  7510.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     847.98 ms /  1281 tokens (    0.66 ms per token,  1510.65 tokens per second)\n",
      "llama_print_timings:        eval time =    1510.13 ms /    46 runs   (   32.83 ms per token,    30.46 tokens per second)\n",
      "llama_print_timings:       total time =    2422.08 ms /  1327 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1247 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.97 ms /    12 runs   (    0.08 ms per token, 12371.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     835.27 ms /  1247 tokens (    0.67 ms per token,  1492.92 tokens per second)\n",
      "llama_print_timings:        eval time =     365.44 ms /    11 runs   (   33.22 ms per token,    30.10 tokens per second)\n",
      "llama_print_timings:       total time =    1215.57 ms /  1258 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1323 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.04 ms /    47 runs   (    0.09 ms per token, 11627.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =     882.12 ms /  1323 tokens (    0.67 ms per token,  1499.80 tokens per second)\n",
      "llama_print_timings:        eval time =    1516.55 ms /    46 runs   (   32.97 ms per token,    30.33 tokens per second)\n",
      "llama_print_timings:       total time =    2447.86 ms /  1369 tokens\n",
      "Llama.generate: 296 prefix-match hit, remaining 1030 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.01 ms /    12 runs   (    0.08 ms per token, 11857.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     739.46 ms /  1030 tokens (    0.72 ms per token,  1392.91 tokens per second)\n",
      "llama_print_timings:        eval time =     358.69 ms /    11 runs   (   32.61 ms per token,    30.67 tokens per second)\n",
      "llama_print_timings:       total time =    1112.06 ms /  1041 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1291 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.37 ms /    43 runs   (    0.10 ms per token,  9830.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     862.03 ms /  1291 tokens (    0.67 ms per token,  1497.63 tokens per second)\n",
      "llama_print_timings:        eval time =    1371.93 ms /    42 runs   (   32.66 ms per token,    30.61 tokens per second)\n",
      "llama_print_timings:       total time =    2284.58 ms /  1333 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 1324 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.33 ms /    12 runs   (    0.11 ms per token,  9049.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =     895.55 ms /  1324 tokens (    0.68 ms per token,  1478.42 tokens per second)\n",
      "llama_print_timings:        eval time =     387.43 ms /    11 runs   (   35.22 ms per token,    28.39 tokens per second)\n",
      "llama_print_timings:       total time =    1297.47 ms /  1335 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 1342 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.16 ms /    47 runs   (    0.09 ms per token, 11289.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     862.36 ms /  1342 tokens (    0.64 ms per token,  1556.20 tokens per second)\n",
      "llama_print_timings:        eval time =    1511.89 ms /    46 runs   (   32.87 ms per token,    30.43 tokens per second)\n",
      "llama_print_timings:       total time =    2425.29 ms /  1388 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1256 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.92 ms /    12 runs   (    0.08 ms per token, 13015.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     815.20 ms /  1256 tokens (    0.65 ms per token,  1540.73 tokens per second)\n",
      "llama_print_timings:        eval time =     348.38 ms /    11 runs   (   31.67 ms per token,    31.57 tokens per second)\n",
      "llama_print_timings:       total time =    1176.72 ms /  1267 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1112 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       5.03 ms /    47 runs   (    0.11 ms per token,  9345.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     735.89 ms /  1112 tokens (    0.66 ms per token,  1511.10 tokens per second)\n",
      "llama_print_timings:        eval time =    1517.57 ms /    46 runs   (   32.99 ms per token,    30.31 tokens per second)\n",
      "llama_print_timings:       total time =    2313.18 ms /  1158 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1072 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.46 ms /    12 runs   (    0.12 ms per token,  8196.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     738.01 ms /  1072 tokens (    0.69 ms per token,  1452.55 tokens per second)\n",
      "llama_print_timings:        eval time =     377.61 ms /    11 runs   (   34.33 ms per token,    29.13 tokens per second)\n",
      "llama_print_timings:       total time =    1132.66 ms /  1083 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1353 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       3.92 ms /    43 runs   (    0.09 ms per token, 10958.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     892.32 ms /  1353 tokens (    0.66 ms per token,  1516.27 tokens per second)\n",
      "llama_print_timings:        eval time =    1380.01 ms /    42 runs   (   32.86 ms per token,    30.43 tokens per second)\n",
      "llama_print_timings:       total time =    2321.79 ms /  1395 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1062 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.16 ms /    12 runs   (    0.10 ms per token, 10309.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     724.03 ms /  1062 tokens (    0.68 ms per token,  1466.79 tokens per second)\n",
      "llama_print_timings:        eval time =     370.46 ms /    11 runs   (   33.68 ms per token,    29.69 tokens per second)\n",
      "llama_print_timings:       total time =    1109.49 ms /  1073 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1333 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       4.28 ms /    47 runs   (    0.09 ms per token, 10978.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =     881.27 ms /  1333 tokens (    0.66 ms per token,  1512.60 tokens per second)\n",
      "llama_print_timings:        eval time =    1505.70 ms /    46 runs   (   32.73 ms per token,    30.55 tokens per second)\n",
      "llama_print_timings:       total time =    2438.87 ms /  1379 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1078 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.19 ms /    12 runs   (    0.10 ms per token, 10109.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     736.59 ms /  1078 tokens (    0.68 ms per token,  1463.49 tokens per second)\n",
      "llama_print_timings:        eval time =     369.80 ms /    11 runs   (   33.62 ms per token,    29.75 tokens per second)\n",
      "llama_print_timings:       total time =    1122.16 ms /  1089 tokens\n",
      "Llama.generate: 136 prefix-match hit, remaining 1187 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Round 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.56 ms /    12 runs   (    0.13 ms per token,  7677.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     789.49 ms /  1187 tokens (    0.67 ms per token,  1503.50 tokens per second)\n",
      "llama_print_timings:        eval time =     378.44 ms /    11 runs   (   34.40 ms per token,    29.07 tokens per second)\n",
      "llama_print_timings:       total time =    1185.92 ms /  1198 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1350 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.08 ms /    12 runs   (    0.09 ms per token, 11131.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     897.25 ms /  1350 tokens (    0.66 ms per token,  1504.60 tokens per second)\n",
      "llama_print_timings:        eval time =     379.71 ms /    11 runs   (   34.52 ms per token,    28.97 tokens per second)\n",
      "llama_print_timings:       total time =    1291.27 ms /  1361 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1265 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.64 ms /    12 runs   (    0.14 ms per token,  7339.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     842.38 ms /  1265 tokens (    0.67 ms per token,  1501.70 tokens per second)\n",
      "llama_print_timings:        eval time =     390.05 ms /    11 runs   (   35.46 ms per token,    28.20 tokens per second)\n",
      "llama_print_timings:       total time =    1251.16 ms /  1276 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1253 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.19 ms /    12 runs   (    0.10 ms per token, 10058.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     831.62 ms /  1253 tokens (    0.66 ms per token,  1506.69 tokens per second)\n",
      "llama_print_timings:        eval time =     371.54 ms /    11 runs   (   33.78 ms per token,    29.61 tokens per second)\n",
      "llama_print_timings:       total time =    1218.66 ms /  1264 tokens\n",
      "Llama.generate: 136 prefix-match hit, remaining 1260 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.20 ms /    12 runs   (    0.10 ms per token, 10041.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     842.78 ms /  1260 tokens (    0.67 ms per token,  1495.06 tokens per second)\n",
      "llama_print_timings:        eval time =     377.21 ms /    11 runs   (   34.29 ms per token,    29.16 tokens per second)\n",
      "llama_print_timings:       total time =    1235.78 ms /  1271 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1307 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.30 ms /    12 runs   (    0.11 ms per token,  9259.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     884.98 ms /  1307 tokens (    0.68 ms per token,  1476.88 tokens per second)\n",
      "llama_print_timings:        eval time =     379.33 ms /    11 runs   (   34.48 ms per token,    29.00 tokens per second)\n",
      "llama_print_timings:       total time =    1281.94 ms /  1318 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1265 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.27 ms /    12 runs   (    0.11 ms per token,  9419.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     840.14 ms /  1265 tokens (    0.66 ms per token,  1505.71 tokens per second)\n",
      "llama_print_timings:        eval time =     371.32 ms /    11 runs   (   33.76 ms per token,    29.62 tokens per second)\n",
      "llama_print_timings:       total time =    1227.20 ms /  1276 tokens\n",
      "Llama.generate: 136 prefix-match hit, remaining 1189 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.96 ms /    12 runs   (    0.08 ms per token, 12448.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     815.66 ms /  1189 tokens (    0.69 ms per token,  1457.71 tokens per second)\n",
      "llama_print_timings:        eval time =     377.83 ms /    11 runs   (   34.35 ms per token,    29.11 tokens per second)\n",
      "llama_print_timings:       total time =    1208.34 ms /  1200 tokens\n",
      "Llama.generate: 355 prefix-match hit, remaining 960 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.30 ms /    12 runs   (    0.11 ms per token,  9216.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     649.85 ms /   960 tokens (    0.68 ms per token,  1477.27 tokens per second)\n",
      "llama_print_timings:        eval time =     384.41 ms /    11 runs   (   34.95 ms per token,    28.62 tokens per second)\n",
      "llama_print_timings:       total time =    1049.51 ms /   971 tokens\n",
      "Llama.generate: 355 prefix-match hit, remaining 991 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.80 ms /    12 runs   (    0.15 ms per token,  6655.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     669.61 ms /   991 tokens (    0.68 ms per token,  1479.96 tokens per second)\n",
      "llama_print_timings:        eval time =     372.94 ms /    11 runs   (   33.90 ms per token,    29.49 tokens per second)\n",
      "llama_print_timings:       total time =    1061.66 ms /  1002 tokens\n",
      "Llama.generate: 355 prefix-match hit, remaining 961 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.36 ms /    12 runs   (    0.11 ms per token,  8804.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     652.61 ms /   961 tokens (    0.68 ms per token,  1472.55 tokens per second)\n",
      "llama_print_timings:        eval time =     355.29 ms /    11 runs   (   32.30 ms per token,    30.96 tokens per second)\n",
      "llama_print_timings:       total time =    1022.47 ms /   972 tokens\n",
      "Llama.generate: 136 prefix-match hit, remaining 1181 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.18 ms /    12 runs   (    0.10 ms per token, 10152.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     778.81 ms /  1181 tokens (    0.66 ms per token,  1516.42 tokens per second)\n",
      "llama_print_timings:        eval time =     380.36 ms /    11 runs   (   34.58 ms per token,    28.92 tokens per second)\n",
      "llama_print_timings:       total time =    1175.17 ms /  1192 tokens\n",
      "Llama.generate: 287 prefix-match hit, remaining 1059 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.99 ms /    12 runs   (    0.08 ms per token, 12170.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     738.86 ms /  1059 tokens (    0.70 ms per token,  1433.29 tokens per second)\n",
      "llama_print_timings:        eval time =     369.73 ms /    11 runs   (   33.61 ms per token,    29.75 tokens per second)\n",
      "llama_print_timings:       total time =    1122.07 ms /  1070 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 1315 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.61 ms /    12 runs   (    0.13 ms per token,  7439.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     855.83 ms /  1315 tokens (    0.65 ms per token,  1536.52 tokens per second)\n",
      "llama_print_timings:        eval time =     377.85 ms /    11 runs   (   34.35 ms per token,    29.11 tokens per second)\n",
      "llama_print_timings:       total time =    1249.94 ms /  1326 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 1306 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.44 ms /    12 runs   (    0.12 ms per token,  8327.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     850.34 ms /  1306 tokens (    0.65 ms per token,  1535.86 tokens per second)\n",
      "llama_print_timings:        eval time =     379.31 ms /    11 runs   (   34.48 ms per token,    29.00 tokens per second)\n",
      "llama_print_timings:       total time =    1244.70 ms /  1317 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1247 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.36 ms /    12 runs   (    0.11 ms per token,  8843.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     800.75 ms /  1247 tokens (    0.64 ms per token,  1557.30 tokens per second)\n",
      "llama_print_timings:        eval time =     368.05 ms /    11 runs   (   33.46 ms per token,    29.89 tokens per second)\n",
      "llama_print_timings:       total time =    1183.10 ms /  1258 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1260 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.28 ms /    12 runs   (    0.11 ms per token,  9353.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     805.90 ms /  1260 tokens (    0.64 ms per token,  1563.46 tokens per second)\n",
      "llama_print_timings:        eval time =     365.07 ms /    11 runs   (   33.19 ms per token,    30.13 tokens per second)\n",
      "llama_print_timings:       total time =    1184.71 ms /  1271 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1274 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.47 ms /    12 runs   (    0.12 ms per token,  8146.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     813.67 ms /  1274 tokens (    0.64 ms per token,  1565.74 tokens per second)\n",
      "llama_print_timings:        eval time =     371.47 ms /    11 runs   (   33.77 ms per token,    29.61 tokens per second)\n",
      "llama_print_timings:       total time =    1201.76 ms /  1285 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1119 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.46 ms /    12 runs   (    0.12 ms per token,  8219.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     758.75 ms /  1119 tokens (    0.68 ms per token,  1474.78 tokens per second)\n",
      "llama_print_timings:        eval time =     363.71 ms /    11 runs   (   33.06 ms per token,    30.24 tokens per second)\n",
      "llama_print_timings:       total time =    1139.23 ms /  1130 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1075 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.26 ms /    12 runs   (    0.10 ms per token,  9538.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =     718.81 ms /  1075 tokens (    0.67 ms per token,  1495.53 tokens per second)\n",
      "llama_print_timings:        eval time =     360.67 ms /    11 runs   (   32.79 ms per token,    30.50 tokens per second)\n",
      "llama_print_timings:       total time =    1094.49 ms /  1086 tokens\n",
      "Llama.generate: 401 prefix-match hit, remaining 945 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.49 ms /    12 runs   (    0.12 ms per token,  8069.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     623.51 ms /   945 tokens (    0.66 ms per token,  1515.61 tokens per second)\n",
      "llama_print_timings:        eval time =     371.92 ms /    11 runs   (   33.81 ms per token,    29.58 tokens per second)\n",
      "llama_print_timings:       total time =    1011.00 ms /   956 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1292 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.99 ms /    12 runs   (    0.08 ms per token, 12060.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     855.70 ms /  1292 tokens (    0.66 ms per token,  1509.87 tokens per second)\n",
      "llama_print_timings:        eval time =     362.64 ms /    11 runs   (   32.97 ms per token,    30.33 tokens per second)\n",
      "llama_print_timings:       total time =    1231.23 ms /  1303 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1260 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.36 ms /    12 runs   (    0.11 ms per token,  8830.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     822.87 ms /  1260 tokens (    0.65 ms per token,  1531.23 tokens per second)\n",
      "llama_print_timings:        eval time =     369.66 ms /    11 runs   (   33.61 ms per token,    29.76 tokens per second)\n",
      "llama_print_timings:       total time =    1208.15 ms /  1271 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1254 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.55 ms /    12 runs   (    0.13 ms per token,  7761.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     824.22 ms /  1254 tokens (    0.66 ms per token,  1521.44 tokens per second)\n",
      "llama_print_timings:        eval time =     364.20 ms /    11 runs   (   33.11 ms per token,    30.20 tokens per second)\n",
      "llama_print_timings:       total time =    1205.17 ms /  1265 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1252 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.26 ms /    12 runs   (    0.10 ms per token,  9523.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     816.89 ms /  1252 tokens (    0.65 ms per token,  1532.65 tokens per second)\n",
      "llama_print_timings:        eval time =     379.20 ms /    11 runs   (   34.47 ms per token,    29.01 tokens per second)\n",
      "llama_print_timings:       total time =    1211.52 ms /  1263 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1087 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.73 ms /    12 runs   (    0.14 ms per token,  6932.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     714.38 ms /  1087 tokens (    0.66 ms per token,  1521.61 tokens per second)\n",
      "llama_print_timings:        eval time =     370.51 ms /    11 runs   (   33.68 ms per token,    29.69 tokens per second)\n",
      "llama_print_timings:       total time =    1101.75 ms /  1098 tokens\n",
      "Llama.generate: 464 prefix-match hit, remaining 868 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.56 ms /    12 runs   (    0.13 ms per token,  7707.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     574.82 ms /   868 tokens (    0.66 ms per token,  1510.04 tokens per second)\n",
      "llama_print_timings:        eval time =     378.50 ms /    11 runs   (   34.41 ms per token,    29.06 tokens per second)\n",
      "llama_print_timings:       total time =     969.12 ms /   879 tokens\n",
      "Llama.generate: 580 prefix-match hit, remaining 752 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.46 ms /    12 runs   (    0.12 ms per token,  8230.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     510.30 ms /   752 tokens (    0.68 ms per token,  1473.65 tokens per second)\n",
      "llama_print_timings:        eval time =     377.24 ms /    11 runs   (   34.29 ms per token,    29.16 tokens per second)\n",
      "llama_print_timings:       total time =     904.51 ms /   763 tokens\n",
      "Llama.generate: 182 prefix-match hit, remaining 1129 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Round 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       2.06 ms /    12 runs   (    0.17 ms per token,  5811.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     748.77 ms /  1129 tokens (    0.66 ms per token,  1507.81 tokens per second)\n",
      "llama_print_timings:        eval time =     364.27 ms /    11 runs   (   33.12 ms per token,    30.20 tokens per second)\n",
      "llama_print_timings:       total time =    1132.39 ms /  1140 tokens\n",
      "Llama.generate: 182 prefix-match hit, remaining 1145 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.00 ms /    12 runs   (    0.08 ms per token, 12000.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     747.56 ms /  1145 tokens (    0.65 ms per token,  1531.64 tokens per second)\n",
      "llama_print_timings:        eval time =     375.64 ms /    11 runs   (   34.15 ms per token,    29.28 tokens per second)\n",
      "llama_print_timings:       total time =    1135.22 ms /  1156 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1234 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.17 ms /    12 runs   (    0.10 ms per token, 10265.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     801.19 ms /  1234 tokens (    0.65 ms per token,  1540.21 tokens per second)\n",
      "llama_print_timings:        eval time =     364.41 ms /    11 runs   (   33.13 ms per token,    30.19 tokens per second)\n",
      "llama_print_timings:       total time =    1178.82 ms /  1245 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1289 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.96 ms /    12 runs   (    0.08 ms per token, 12474.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     850.36 ms /  1289 tokens (    0.66 ms per token,  1515.83 tokens per second)\n",
      "llama_print_timings:        eval time =     370.79 ms /    11 runs   (   33.71 ms per token,    29.67 tokens per second)\n",
      "llama_print_timings:       total time =    1234.33 ms /  1300 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1069 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.21 ms /    12 runs   (    0.10 ms per token,  9917.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     719.75 ms /  1069 tokens (    0.67 ms per token,  1485.23 tokens per second)\n",
      "llama_print_timings:        eval time =     363.60 ms /    11 runs   (   33.05 ms per token,    30.25 tokens per second)\n",
      "llama_print_timings:       total time =    1097.68 ms /  1080 tokens\n",
      "Llama.generate: 355 prefix-match hit, remaining 1024 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.08 ms /    12 runs   (    0.09 ms per token, 11121.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     660.14 ms /  1024 tokens (    0.64 ms per token,  1551.19 tokens per second)\n",
      "llama_print_timings:        eval time =     364.74 ms /    11 runs   (   33.16 ms per token,    30.16 tokens per second)\n",
      "llama_print_timings:       total time =    1038.92 ms /  1035 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1089 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.15 ms /    12 runs   (    0.10 ms per token, 10443.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     736.18 ms /  1089 tokens (    0.68 ms per token,  1479.27 tokens per second)\n",
      "llama_print_timings:        eval time =     372.66 ms /    11 runs   (   33.88 ms per token,    29.52 tokens per second)\n",
      "llama_print_timings:       total time =    1125.57 ms /  1100 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1374 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.31 ms /    12 runs   (    0.11 ms per token,  9188.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     917.02 ms /  1374 tokens (    0.67 ms per token,  1498.33 tokens per second)\n",
      "llama_print_timings:        eval time =     375.84 ms /    11 runs   (   34.17 ms per token,    29.27 tokens per second)\n",
      "llama_print_timings:       total time =    1309.29 ms /  1385 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1309 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.94 ms /    12 runs   (    0.08 ms per token, 12752.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     868.94 ms /  1309 tokens (    0.66 ms per token,  1506.43 tokens per second)\n",
      "llama_print_timings:        eval time =     375.93 ms /    11 runs   (   34.18 ms per token,    29.26 tokens per second)\n",
      "llama_print_timings:       total time =    1257.87 ms /  1320 tokens\n",
      "Llama.generate: 296 prefix-match hit, remaining 1072 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.12 ms /    12 runs   (    0.09 ms per token, 10685.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     740.09 ms /  1072 tokens (    0.69 ms per token,  1448.47 tokens per second)\n",
      "llama_print_timings:        eval time =     366.37 ms /    11 runs   (   33.31 ms per token,    30.02 tokens per second)\n",
      "llama_print_timings:       total time =    1121.33 ms /  1083 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1223 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.05 ms /    12 runs   (    0.09 ms per token, 11472.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =     817.68 ms /  1223 tokens (    0.67 ms per token,  1495.70 tokens per second)\n",
      "llama_print_timings:        eval time =     381.35 ms /    11 runs   (   34.67 ms per token,    28.84 tokens per second)\n",
      "llama_print_timings:       total time =    1213.00 ms /  1234 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1256 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.07 ms /    12 runs   (    0.09 ms per token, 11194.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     840.70 ms /  1256 tokens (    0.67 ms per token,  1493.99 tokens per second)\n",
      "llama_print_timings:        eval time =     376.85 ms /    11 runs   (   34.26 ms per token,    29.19 tokens per second)\n",
      "llama_print_timings:       total time =    1231.28 ms /  1267 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 1295 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.86 ms /    12 runs   (    0.07 ms per token, 14018.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     849.95 ms /  1295 tokens (    0.66 ms per token,  1523.62 tokens per second)\n",
      "llama_print_timings:        eval time =     364.86 ms /    11 runs   (   33.17 ms per token,    30.15 tokens per second)\n",
      "llama_print_timings:       total time =    1228.12 ms /  1306 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 1339 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.26 ms /    12 runs   (    0.10 ms per token,  9523.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     897.81 ms /  1339 tokens (    0.67 ms per token,  1491.40 tokens per second)\n",
      "llama_print_timings:        eval time =     372.41 ms /    11 runs   (   33.86 ms per token,    29.54 tokens per second)\n",
      "llama_print_timings:       total time =    1286.22 ms /  1350 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1119 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Round 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.49 ms /    12 runs   (    0.12 ms per token,  8069.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     765.07 ms /  1119 tokens (    0.68 ms per token,  1462.60 tokens per second)\n",
      "llama_print_timings:        eval time =     376.77 ms /    11 runs   (   34.25 ms per token,    29.20 tokens per second)\n",
      "llama_print_timings:       total time =    1159.28 ms /  1130 tokens\n",
      "Llama.generate: 182 prefix-match hit, remaining 1169 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.98 ms /    12 runs   (    0.08 ms per token, 12244.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     801.22 ms /  1169 tokens (    0.69 ms per token,  1459.03 tokens per second)\n",
      "llama_print_timings:        eval time =     366.36 ms /    11 runs   (   33.31 ms per token,    30.02 tokens per second)\n",
      "llama_print_timings:       total time =    1180.70 ms /  1180 tokens\n",
      "Llama.generate: 182 prefix-match hit, remaining 1162 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.23 ms /    12 runs   (    0.10 ms per token,  9732.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     790.68 ms /  1162 tokens (    0.68 ms per token,  1469.62 tokens per second)\n",
      "llama_print_timings:        eval time =     369.31 ms /    11 runs   (   33.57 ms per token,    29.79 tokens per second)\n",
      "llama_print_timings:       total time =    1175.05 ms /  1173 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 1321 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.21 ms /    12 runs   (    0.10 ms per token,  9892.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     870.03 ms /  1321 tokens (    0.66 ms per token,  1518.35 tokens per second)\n",
      "llama_print_timings:        eval time =     367.40 ms /    11 runs   (   33.40 ms per token,    29.94 tokens per second)\n",
      "llama_print_timings:       total time =    1251.23 ms /  1332 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 1307 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.98 ms /    12 runs   (    0.08 ms per token, 12295.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     862.10 ms /  1307 tokens (    0.66 ms per token,  1516.06 tokens per second)\n",
      "llama_print_timings:        eval time =     364.68 ms /    11 runs   (   33.15 ms per token,    30.16 tokens per second)\n",
      "llama_print_timings:       total time =    1239.88 ms /  1318 tokens\n",
      "Llama.generate: 462 prefix-match hit, remaining 849 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.33 ms /    12 runs   (    0.11 ms per token,  9009.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     581.04 ms /   849 tokens (    0.68 ms per token,  1461.17 tokens per second)\n",
      "llama_print_timings:        eval time =     360.32 ms /    11 runs   (   32.76 ms per token,    30.53 tokens per second)\n",
      "llama_print_timings:       total time =     957.14 ms /   860 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1070 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       2.02 ms /    12 runs   (    0.17 ms per token,  5946.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     718.14 ms /  1070 tokens (    0.67 ms per token,  1489.95 tokens per second)\n",
      "llama_print_timings:        eval time =     360.38 ms /    11 runs   (   32.76 ms per token,    30.52 tokens per second)\n",
      "llama_print_timings:       total time =    1100.42 ms /  1081 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1259 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Round 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.67 ms /    12 runs   (    0.14 ms per token,  7189.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =     830.36 ms /  1259 tokens (    0.66 ms per token,  1516.20 tokens per second)\n",
      "llama_print_timings:        eval time =     368.62 ms /    11 runs   (   33.51 ms per token,    29.84 tokens per second)\n",
      "llama_print_timings:       total time =    1218.17 ms /  1270 tokens\n",
      "Llama.generate: 71 prefix-match hit, remaining 1299 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.42 ms /    12 runs   (    0.12 ms per token,  8438.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =     866.07 ms /  1299 tokens (    0.67 ms per token,  1499.88 tokens per second)\n",
      "llama_print_timings:        eval time =     376.54 ms /    11 runs   (   34.23 ms per token,    29.21 tokens per second)\n",
      "llama_print_timings:       total time =    1259.47 ms /  1310 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1129 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.39 ms /    12 runs   (    0.12 ms per token,  8620.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     747.28 ms /  1129 tokens (    0.66 ms per token,  1510.82 tokens per second)\n",
      "llama_print_timings:        eval time =     363.87 ms /    11 runs   (   33.08 ms per token,    30.23 tokens per second)\n",
      "llama_print_timings:       total time =    1126.49 ms /  1140 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1077 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.77 ms /    12 runs   (    0.15 ms per token,  6795.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     730.34 ms /  1077 tokens (    0.68 ms per token,  1474.65 tokens per second)\n",
      "llama_print_timings:        eval time =     360.52 ms /    11 runs   (   32.77 ms per token,    30.51 tokens per second)\n",
      "llama_print_timings:       total time =    1110.92 ms /  1088 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1086 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Round 6 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       1.35 ms /    12 runs   (    0.11 ms per token,  8915.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     718.61 ms /  1086 tokens (    0.66 ms per token,  1511.25 tokens per second)\n",
      "llama_print_timings:        eval time =     368.88 ms /    11 runs   (   33.53 ms per token,    29.82 tokens per second)\n",
      "llama_print_timings:       total time =    1103.19 ms /  1097 tokens\n",
      "Llama.generate: 250 prefix-match hit, remaining 1112 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =    1267.51 ms\n",
      "llama_print_timings:      sample time =       0.91 ms /    12 runs   (    0.08 ms per token, 13129.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     743.49 ms /  1112 tokens (    0.67 ms per token,  1495.65 tokens per second)\n",
      "llama_print_timings:        eval time =     357.13 ms /    11 runs   (   32.47 ms per token,    30.80 tokens per second)\n",
      "llama_print_timings:       total time =    1113.68 ms /  1123 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final round winners (Top 2 ideas): ['Pollen-based bioinks for 3D printing and biomedical applications', '**Biocompatible scaffolds:** Pollen-based scaffolds could be developed for tissue engineering applications, providing a more biocompatible and sustainable alternative to synthetic scaffolds.']\n",
      "Results (including rounds) exported to convergedideas_elim.csv.\n",
      "Ratings from the first round exported to convergedideas_rate.csv.\n"
     ]
    }
   ],
   "source": [
    "from protocols.idea_con import pairwise_elim_and_rate\n",
    "\n",
    "# Filename to store the elimination results and final winners\n",
    "outputfilename = f'{filetagline}_elim.csv'\n",
    "# Filename to store the ratings collected in the first round\n",
    "ratings_outputfile = f'{filetagline}_rate.csv'\n",
    "\n",
    "# Run the pairwise elimination and rating function\n",
    "pairwise_elim_and_rate(filename, prompt, round_limit, outputfilename, ratings_outputfile,query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c25cff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plantmat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ff8e1b-6969-4972-96c6-d2709be2c023",
   "metadata": {},
   "source": [
    "# BioLLM x Plants - Idea Mining, Divergent-Convergent Script\n",
    "\n",
    "Rachel K. Luu, Ming Dao, Subra Suresh, Markus J. Buehler (2025) ENHANCING SCIENTIFIC INNOVATION IN LLMS: A FRAMEWORK APPLIED TO PLANT MECHANICS RESEARCH [full reference to be updated to be included here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3779f88b",
   "metadata": {},
   "source": [
    "## Load Model Functions\n",
    "\n",
    "For the divergent generation phase, BioinspiredLLM quantized to 8bit is used. For the convergent evaluation phase, Llama-3.1-8b-instruct quantized to 8bit is used. Depending on your system, you may need to load these models separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434640b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from typing import List, Optional, Sequence\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"Llama 3.1 Prompt Template\"\"\"\n",
    "\n",
    "def completion_to_prompt(completion):\n",
    "    return \"<|start_header_id|>system<|end_header_id|>\\n<eot_id>\\n<|start_header_id|>user<|end_header_id|>\\n\" + \\\n",
    "           f\"{completion}<eot_id>\\n<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "    prompt = \"<|start_header_id|>system<|end_header_id|>\\n<eot_id>\\n\"  \n",
    "    for message in messages:\n",
    "        if message.role == \"system\":\n",
    "            prompt += f\"system message<eot_id>\\n\"\n",
    "        elif message.role == \"user\":\n",
    "            prompt += f\"<|start_header_id|>user<|end_header_id|>\\n{message.content}<eot_id>\\n\"\n",
    "        elif message.role == \"assistant\":\n",
    "            prompt += f\"<|start_header_id|>assistant<|end_header_id|>\\n{message.content}<eot_id>\\n\"\n",
    "    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    \n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ea25e",
   "metadata": {},
   "source": [
    "## Load BioLLM Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e359c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"https://huggingface.co/rachelkluu/Llama3.1-8b-Instruct-CPT-SFT-DPO-09022024-Q8_0-GGUF/resolve/main/llama3.1-8b-instruct-cpt-sft-dpo-09022024-q8_0.gguf\"\n",
    "bioinspiredllm_q8 = LlamaCPP(\n",
    "    model_url=model_url,\n",
    "    model_path=None,\n",
    "    temperature=1,\n",
    "    max_new_tokens=2048,\n",
    "    context_window=16000,\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=False,\n",
    "     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a19cde",
   "metadata": {},
   "source": [
    "## Load Llama3.1 Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d06c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"https://huggingface.co/rachelkluu/Meta-Llama-3.1-8B-Instruct-Q8_0-GGUF/resolve/main/meta-llama-3.1-8b-instruct-q8_0.gguf\"\n",
    "llama31_q8 = LlamaCPP(\n",
    "    model_url=model_url,\n",
    "    model_path=None,\n",
    "    temperature=.1,\n",
    "    max_new_tokens=5000,\n",
    "    context_window=16000,\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    "     \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0880eb78",
   "metadata": {},
   "source": [
    "## Divergent Generation\n",
    "For the divergent generation phase, BioinspiredLLM quantized to 8bit is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd707f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "\n",
    "Settings.llm = bioinspiredllm_q8 # BIOLLM USED HERE\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"./PlantPapers/\"   # FOLDER TO PAPERS OF INTEREST\n",
    ").load_data()\n",
    "\n",
    "Settings.chunk_size = 128\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = vector_index.as_query_engine(response_mode=\"compact\", similarity_top_k=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f045f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def extract_bullet_points(response_text):\n",
    "    lines = response_text.split('\\n')\n",
    "    bullet_points = set()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(('- ', 'â€¢ ', '* ')):\n",
    "            bullet_points.add(line[2:].strip())\n",
    "        elif line and line[0].isdigit() and line[1:3] == '. ':\n",
    "            bullet_points.add(line[3:].strip())\n",
    "        elif line.startswith('[') and line[1].isdigit() and line[2] == ']':\n",
    "            bullet_points.add(line[3:].strip())\n",
    "    return list(bullet_points)\n",
    "\n",
    "def are_similar(a, b, threshold=0.8):\n",
    "    return SequenceMatcher(None, a, b).ratio() > threshold\n",
    "\n",
    "def sample_bullets(num_generations, num_ideas, prompt):\n",
    "    \"\"\"Samples the LLM num_generations number of times, scraping the bullet points of ideas generated\"\"\"\n",
    "    all_bullet_points = set()\n",
    "    bullet_points_count = []\n",
    "    bullet_points_per_generation = []\n",
    "    data_for_df =[]\n",
    "    for gen_num in range(num_generations):\n",
    "        txt = f\"{prompt} Be creative. Concisely brainstorm {num_ideas} different ideas into a bullet point list.\"\n",
    "        response = query_engine.query(txt)\n",
    "        bullet_points = extract_bullet_points(response.response)\n",
    "        all_bullet_points.update(bullet_points)\n",
    "        for bullet_point in bullet_points:\n",
    "            data_for_df.append({\"Prompt\": prompt, \"Idea\": bullet_point})\n",
    "        bullcount = len(all_bullet_points)\n",
    "    df = pd.DataFrame(data_for_df, columns=[\"Prompt\", \"Idea\"])\n",
    "    return df, all_bullet_points, bullcount\n",
    "\n",
    "\n",
    "def filter_ideas(new_ideas, unique_ideas, similarity_threshold=0.8):\n",
    "    \"\"\"Filters ideas in the list by similarity, can adjust similarity threshold\"\"\"\n",
    "    filtered_ideas = []\n",
    "    for idea in new_ideas:\n",
    "        is_unique = all(not are_similar(idea, existing_idea, similarity_threshold) for existing_idea in unique_ideas)\n",
    "        if is_unique and all(not are_similar(idea, filtered_idea, similarity_threshold) for filtered_idea in filtered_ideas):\n",
    "            filtered_ideas.append(idea)\n",
    "    return filtered_ideas\n",
    "\n",
    "def filter_unique_ideas(df_existing, df_new, similarity_threshold=0.8):\n",
    "    unique_ideas = df_existing['Idea'].tolist()\n",
    "    new_ideas = df_new['Idea'].tolist()\n",
    "    filtered_ideas = filter_ideas(new_ideas, unique_ideas, similarity_threshold)\n",
    "    df_filtered = df_new[df_new['Idea'].isin(filtered_ideas)]\n",
    "    return pd.concat([df_existing, df_filtered], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89317726",
   "metadata": {},
   "source": [
    "### Divergent Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c356fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Beyond hemorrhage control, where else could the high absorption properties of pollen-based cryogels be effectively applied?\"\n",
    "num_per_gen = \"\" #can optionally specify how many ideas per generation\n",
    "sim_thres = 0.7 #adjusts the similarity threshold, with values closer to 1 being greater similarity \n",
    "num_ideas = 100 #number of ideas desired in total\n",
    "\n",
    "##############################\n",
    "\n",
    "finaldf = pd.DataFrame({\n",
    "    \"Prompt\":[],\n",
    "    \"Idea\":[],\n",
    "})\n",
    "\n",
    "while len(finaldf) < num_ideas:\n",
    "    gendf, bullets, bullcount = sample_bullets(1, num_per_gen, prompt) \n",
    "    print(bullets)\n",
    "    print(f\"{bullcount} Ideas were generated\")\n",
    "    \n",
    "    finaldf = filter_unique_ideas(finaldf, gendf, similarity_threshold= sim_thres)\n",
    "    print(finaldf)\n",
    "    print(f\"Unique rows were added. Current number of rows: {len(finaldf)}\")\n",
    "\n",
    "finaldf \n",
    "\n",
    "finaldf.to_csv(f'{prompt}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa4cc5e",
   "metadata": {},
   "source": [
    "## Convergent Evaluation\n",
    "For the convergent evaluation phase, Llama-3.1-8b-instruct quantized to 8bit is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d79824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "\n",
    "Settings.llm = llama31_q8 \n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"./PlantPapers/\"   # FOLDER TO PAPERS OF INTEREST\n",
    ").load_data()\n",
    "\n",
    "Settings.chunk_size = 128\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = vector_index.as_query_engine(response_mode=\"compact\", similarity_top_k=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd14c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def clean_json_string(json_string):\n",
    "    cleaned_string = json_string.replace(\"\\n\", \" \")\n",
    "    return cleaned_string\n",
    "\n",
    "def load_ideas_from_files(filenames):\n",
    "    all_ideas = []\n",
    "    for filename in filenames:\n",
    "        df = pd.read_csv(filename)\n",
    "        ideas = df.iloc[:, 1].tolist()\n",
    "        all_ideas.extend(ideas)\n",
    "    return all_ideas\n",
    "\n",
    "\n",
    "def pairwise_elim_and_rate(filenames, prompt, round_limit, outputfilename, ratings_outputfile):\n",
    "    all_ideas = load_ideas_from_files(filenames)\n",
    "    random.shuffle(all_ideas)\n",
    "    round_number = 1\n",
    "    all_rounds_data = []  # List to collect data for each round\n",
    "    ratings_data = []  # List to store ratings data for each idea\n",
    "\n",
    "    while len(all_ideas) > 1 and round_number <= round_limit:\n",
    "        print(f\"--- Round {round_number} ---\")\n",
    "        winners = []\n",
    "        # Compare ideas in pairs\n",
    "        for i in range(0, len(all_ideas), 2):\n",
    "            if i + 1 < len(all_ideas):  \n",
    "                idea_1 = all_ideas[i]\n",
    "                idea_2 = all_ideas[i + 1]\n",
    "\n",
    "                # Only rate the ideas in the first round\n",
    "                if round_number == 1:\n",
    "                    # Prepare the rating prompt for these two ideas\n",
    "                    rating_prompt = f\"\"\"You must critically rate each idea out of 10 for categories: novelty and effectiveness.\n",
    "                    The response must only contain the ratings in the following strict JSON format:\n",
    "                    {{\n",
    "                    \"Idea 1\": {{\"novelty\": X, \"effectiveness\": Y}},\n",
    "                    \"Idea 2\": {{\"novelty\": X, \"effectiveness\": Y}}\n",
    "                    }}\"\"\"\n",
    "                    \n",
    "                    ideas_str = f\"Idea 1: {idea_1}\\nIdea 2: {idea_2}\"\n",
    "                    rating_txt = f\"{rating_prompt}\\n\\n{ideas_str}\"\n",
    "                    rating_response = query_engine.query(rating_txt)\n",
    "                    cleaned_rating_response = clean_json_string(rating_response.response)\n",
    "\n",
    "                    try:\n",
    "                        ratings = json.loads(cleaned_rating_response)\n",
    "                        ratings_data.append({\n",
    "                            'Idea': idea_1,\n",
    "                            'Novelty': ratings.get('Idea 1', {}).get('novelty', None),\n",
    "                            'Effectiveness': ratings.get('Idea 1', {}).get('effectiveness', None)\n",
    "                        })\n",
    "                        ratings_data.append({\n",
    "                            'Idea': idea_2,\n",
    "                            'Novelty': ratings.get('Idea 2', {}).get('novelty', None),\n",
    "                            'Effectiveness': ratings.get('Idea 2', {}).get('effectiveness', None)\n",
    "                        })\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error decoding JSON: {e}\")\n",
    "                        continue  \n",
    "\n",
    "                compare_txt = f\"\"\"To answer this question: {prompt} Which idea is better based on novelty and effectiveness?\n",
    "                Idea 1: {idea_1} \n",
    "                Idea 2: {idea_2} \n",
    "                Respond with the winner as 'Idea 1' or 'Idea 2' in the following JSON format:\n",
    "                {{ \"winner\": \"Idea 1\" or \"Idea 2\" }}\n",
    "                \"\"\"\n",
    "\n",
    "                comparison_response = query_engine.query(compare_txt)\n",
    "                cleaned_comparison_response = clean_json_string(comparison_response.response)\n",
    "\n",
    "                try:\n",
    "                    result = json.loads(cleaned_comparison_response)\n",
    "                    winner = idea_1 if result[\"winner\"] == \"Idea 1\" else idea_2\n",
    "                    winners.append(winner)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON: {e}\")\n",
    "            \n",
    "            else:\n",
    "                # If there's an odd number of ideas, move the last one directly to the next round\n",
    "                winners.append(all_ideas[i])\n",
    "\n",
    "        # Store the remaining ideas after the round\n",
    "        round_data = [{'Round': round_number, 'Idea': idea} for idea in winners]\n",
    "        all_rounds_data.extend(round_data)\n",
    "\n",
    "        # Move the winners to the next round\n",
    "        all_ideas = winners\n",
    "        random.shuffle(all_ideas)  # Randomize for the next round\n",
    "        round_number += 1\n",
    "\n",
    "    print(f\"Final round winners (Top {len(all_ideas)} ideas): {[idea for idea in all_ideas]}\")\n",
    "    \n",
    "    df_winners = pd.DataFrame(all_ideas)\n",
    "    df_winners['Round'] = round_number - 1  \n",
    "    output_file = outputfilename\n",
    "    df_all_rounds = pd.DataFrame(all_rounds_data)\n",
    "    final_output_df = pd.concat([df_all_rounds, df_winners])\n",
    "    final_output_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Results (including rounds) exported to {output_file}.\")\n",
    "\n",
    "    df_ratings = pd.DataFrame(ratings_data)\n",
    "    df_ratings.to_csv(ratings_outputfile, index=False)\n",
    "    \n",
    "    print(f\"Ratings from the first round exported to {ratings_outputfile}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5180ef2e",
   "metadata": {},
   "source": [
    "### Convergent Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d417e92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Beyond hemorrhage control, where else could the high absorption properties of pollen-based cryogels be effectively applied?\"\n",
    "filename=[f\"{prompt}.csv\",]\n",
    "filetagline = \"PollenAbsorption\"\n",
    "\n",
    "\n",
    "\n",
    "# Maximum number of elimination rounds to perform\n",
    "round_limit = 6\n",
    "\n",
    "# Filename to store the elimination results and final winners\n",
    "outputfilename = f'{filetagline}_elim.csv'\n",
    "\n",
    "# Filename to store the ratings collected in the first round\n",
    "ratings_outputfile = f'{filetagline}_rate.csv'\n",
    "\n",
    "# Run the pairwise elimination and rating function\n",
    "pairwise_elim_and_rate(filename, prompt, round_limit, outputfilename, ratings_outputfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plantmat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

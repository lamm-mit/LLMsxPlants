{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ff8e1b-6969-4972-96c6-d2709be2c023",
   "metadata": {},
   "source": [
    "# BioLLM x Plants - Procedure Design - Multi-Agent\n",
    "\n",
    "Rachel K. Luu, Ming Dao, Subra Suresh, Markus J. Buehler (2025) ENHANCING SCIENTIFIC INNOVATION IN LLMS: A FRAMEWORK APPLIED TO PLANT MECHANICS RESEARCH [full reference to be updated to be included here]\n",
    "\n",
    "Multi-Agent Question-Answer Interaction code developed by Markus J. Buehler using the Llama-Index chat model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cfc916",
   "metadata": {},
   "source": [
    "# Load Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from datasets import IterableDataset\n",
    "from transformers import AutoConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "import markdown2\n",
    "import pdfkit\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import math\n",
    "import numpy as np\n",
    "import unidecode\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import peft\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.llama_cpp.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.response.notebook_utils import display_response\n",
    "from IPython.display import Markdown, display\n",
    "import re\n",
    "pd.set_option('display.max_colwidth', None)  \n",
    "\n",
    "def make_dir_if_needed (dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        # Create directory\n",
    "        os.makedirs(dir_path)\n",
    "        return \"Directory created.\"\n",
    "    else:\n",
    "        return  \"Directory already exists.\"\n",
    "\n",
    "def remove_markdown_symbols(text):\n",
    "    # Remove links\n",
    "    text = re.sub(r'\\[([^\\]]+)\\]\\([^\\)]+\\)', r'\\1', text)\n",
    "    # Remove images\n",
    "    text = re.sub(r'!\\[[^\\]]*\\]\\([^\\)]+\\)', '', text)\n",
    "    # Remove headers\n",
    "    text = re.sub(r'#+\\s', '', text)\n",
    "    # Remove bold and italic\n",
    "    text = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', text)\n",
    "    text = re.sub(r'\\*([^*]+)\\*', r'\\1', text)\n",
    "    text = re.sub(r'__([^_]+)__', r'\\1', text)\n",
    "    text = re.sub(r'_([^_]+)_', r'\\1', text)\n",
    "    # Remove inline code\n",
    "    text = re.sub(r'`([^`]+)`', r'\\1', text)\n",
    "    # Remove blockquotes\n",
    "    text = re.sub(r'^>\\s+', '', text, flags=re.MULTILINE)\n",
    "    # Remove strikethrough\n",
    "    text = re.sub(r'~~(.*?)~~', r'\\1', text)\n",
    "    # Remove code blocks\n",
    "    text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)\n",
    "    # Remove extra newlines\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    # Remove list markers\n",
    "    text = re.sub(r'^[\\*\\-\\+]\\s+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^\\d+\\.\\s+', '', text, flags=re.MULTILINE)\n",
    "    return text.strip()\n",
    "\n",
    "def completion_to_prompt(completion):\n",
    "    return \"<|start_header_id|>system<|end_header_id|>\\n<eot_id>\\n<|start_header_id|>user<|end_header_id|>\\n\" + \\\n",
    "           f\"{completion}<eot_id>\\n<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "    prompt = \"<|start_header_id|>system<|end_header_id|>\\n<eot_id>\\n\"  \n",
    "    for message in messages:\n",
    "        if message.role == \"system\":\n",
    "            prompt += f\"system message<eot_id>\\n\"\n",
    "        elif message.role == \"user\":\n",
    "            prompt += f\"<|start_header_id|>user<|end_header_id|>\\n{message.content}<eot_id>\\n\"\n",
    "        elif message.role == \"assistant\":\n",
    "            prompt += f\"<|start_header_id|>assistant<|end_header_id|>\\n{message.content}<eot_id>\\n\"\n",
    "    prompt += \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def get_chat_engine_from_index_LlamaIndex(llm,index, chat_token_limit=2500,verbose=False, chat_mode=\"context\",\n",
    "                               system_prompt='You are a chatbot, able to have normal interactions, as well as talk about context provided.'):\n",
    "    memory = ChatMemoryBuffer.from_defaults(token_limit=chat_token_limit)\n",
    "    chat_engine = index.as_chat_engine(llm=llm,\n",
    "        chat_mode=chat_mode,\n",
    "        memory=memory,\n",
    "        system_prompt=system_prompt,verbose=verbose,\n",
    "    )\n",
    "    return chat_engine\n",
    "\n",
    "def get_answer_LlamaIndex (llm,\n",
    "                           q, system_prompt=\"You are an expert in materials science.\", chat_engine=None,\n",
    "                        max_new_tokens=1024, #temperature=0.7, \n",
    "                           messages_to_prompt=None,chat_token_limit=2500,chat_mode=\"context\",\n",
    "                completion_to_prompt=None,index=None, verbose=False):\n",
    "    if chat_engine==None:\n",
    "        '''\n",
    "        llm = HuggingFaceLLM(model=model, tokenizer=tokenizer,\n",
    "                     context_window=8192,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "               # model_kwargs={\"quantization_config\": quantization_config},\n",
    "                generate_kwargs={\"temperature\": temperature, \"top_k\": 256, \"top_p\": 0.9,\"do_sample\":\"True\", \"eos_token_id\":eos_token_id},\n",
    "                messages_to_prompt=messages_to_prompt,\n",
    "                completion_to_prompt=completion_to_prompt,\n",
    "                    device_map=\"cuda:0\",system_prompt=system_prompt)\n",
    "        '''\n",
    "        if index != None:\n",
    "            chat_engine=get_chat_engine_from_index_LlamaIndex(llm,index, chat_token_limit=chat_token_limit,verbose=verbose,chat_mode=chat_mode,\n",
    "                                   system_prompt=f'You are a chatbot, able to have normal interactions, as well as talk about data provided. {system_prompt}')\n",
    "        else:\n",
    "            chat_engine = SimpleChatEngine.from_defaults(llm=llm, system_prompt=system_prompt)     \n",
    "    response = chat_engine.stream_chat(q)\n",
    "    for token in response.response_gen:\n",
    "        print(token, end=\"\")\n",
    "    return response.response, chat_engine\n",
    "\n",
    "class ConversationAgent_LlamaIndex:\n",
    "    def __init__(self, llm,\n",
    "                 #model, tokenizer, \n",
    "                 name: str, instructions: str,# context_turns: int = 2,\n",
    "               # temperature=0.3,  max_new_tokens=1024, \n",
    "              #   messages_to_prompt=None, completion_to_prompt=None,\n",
    "                 index=None,chat_token_limit=2500,verbose=False,chat_mode=\"context\",\n",
    "                ):\n",
    "        self._name = name\n",
    "        self._instructions = instructions\n",
    "        self._source_nodes =[]\n",
    "        #self._my_turns = []\n",
    "        '''\n",
    "        self.temperature=temperature\n",
    "        \n",
    "        llm = HuggingFaceLLM(model=model, tokenizer=tokenizer,\n",
    "                     context_window=8192,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "               # model_kwargs={\"quantization_config\": quantization_config},\n",
    "                generate_kwargs={\"temperature\": self.temperature, \"top_k\": 256, \"top_p\": 0.9,\"do_sample\":\"True\", \"eos_token_id\":eos_token_id},\n",
    "                messages_to_prompt=messages_to_prompt,\n",
    "                completion_to_prompt=completion_to_prompt,\n",
    "                device_map=\"auto\",)\n",
    "        '''            \n",
    "        #self.chat_engine = SimpleChatEngine.from_defaults(llm=llm, system_prompt=self._instructions)\n",
    "        if index != None:\n",
    "            print (f\"Set up chat engine, with index, verbose={verbose}, chat_mode={chat_mode}.\")\n",
    "            \n",
    "           # Settings.embed_model = resolve_embed_model(\"local:BAAI/bge-small-en-v1.5\")\n",
    "           # Settings.llm = llm\n",
    "            self.chat_engine=get_chat_engine_from_index_LlamaIndex(llm,index, chat_token_limit=chat_token_limit,verbose=verbose,chat_mode=chat_mode,\n",
    "                       system_prompt=f'You are a chatbot, able to have normal interactions, as well as talk about data provided.\\n\\n{self._instructions}')\n",
    "        else:\n",
    "            self.chat_engine = SimpleChatEngine.from_defaults(llm=llm, system_prompt=self._instructions)\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return self._name\n",
    "    def get_conv(self, ) -> str:\n",
    "        return self.chat_engine.chat_history\n",
    "    def get_source_nodes(self, ) -> str:\n",
    "        return self._source_nodes\n",
    "    def reset_chat(self, ):\n",
    "        self.chat_engine.reset()\n",
    "    def reply(self, question) -> str:\n",
    "        response = self.chat_engine.stream_chat(question   )\n",
    "        for token in response.response_gen:\n",
    "            print(token, end=\"\")\n",
    "        self._source_nodes.append (response.source_nodes)\n",
    "        #self._my_turns.append(response.response)\n",
    "        return response.response, response\n",
    "\n",
    "def conversation_simulator_LlamaIndex(\n",
    "     llm_answer, llm_question,\n",
    "    question_gpt_name='Engineer',answer_gpt_name='Biologist', answer_instructions='You answer correctly.',\n",
    "    question_asker_instructions='You always respond with a tough questions. ',\n",
    "    q='What is bioinspiration?',\n",
    "    total_turns: int = 5,data_dir='./',\n",
    "    marker_ch='>>> ',start_with_q=False,only_last=True, \n",
    "    marker_ch_outer='### ',sample_question='',\n",
    "    answer_index=None,question_index=None, verbose=False,chat_mode=\"context\",chat_token_limit=2500,\n",
    "    include_N_turns_in_question_development=9999,single_shot_question=True,\n",
    "    )-> list[dict[str,str]]:\n",
    "\n",
    "    answer_agent = ConversationAgent_LlamaIndex(llm_answer,  \n",
    "                                                name=answer_gpt_name, instructions=answer_instructions, \n",
    "                                                index=answer_index,verbose=verbose,chat_mode=chat_mode,chat_token_limit=chat_token_limit,\n",
    "                                               )\n",
    "    conversation_turns = []\n",
    "    q_new = q #None\n",
    "    conversation_turns.append(dict(name=question_gpt_name, text=q_new))\n",
    "    print (f\"### {question_gpt_name}: {q}\\n\")\n",
    "    for _ in range(total_turns):\n",
    "        print (f\"### {answer_gpt_name}: \", end=\"\")\n",
    "        last_reply, response = answer_agent.reply(q_new)\n",
    "        conversation_turns.append(dict(name=answer_gpt_name, text=last_reply))\n",
    "        if only_last:\n",
    "            txt= f'Consider this question and response.\\n\\n{marker_ch_outer}Question: {q}\\n\\n{marker_ch_outer} Response: {last_reply}'\n",
    "        else:\n",
    "            NN=include_N_turns_in_question_development\n",
    "            \n",
    "            NN = NN + 1 if NN % 2 else NN  \n",
    "            conv=get_entire_conversation_LlamaIndex(q, conversation_turns[-NN:],marker_ch=marker_ch,start_with_q=start_with_q, question_gpt_name=question_gpt_name)\n",
    "            \n",
    "            txt=f'{marker_ch_outer}Read this conversation between you and {answer_gpt_name}:\\n\\n```{conv}```\\n\\n\"'\n",
    "        if single_shot_question: \n",
    "            q=f\"\"\"{txt}\\n\\n{marker_ch_outer} Very briefly respond to point out how to refine the procedure\"\"\"\n",
    "            print (f\"\\n\\n### {question_gpt_name}: \", end=\"\")\n",
    "            q_new, q_chat=get_answer_LlamaIndex (llm_question,\n",
    "                                            q=q, \n",
    "                                            index=question_index,verbose=verbose,chat_mode=chat_mode,chat_token_limit=chat_token_limit,\n",
    "                 system_prompt=question_asker_instructions)\n",
    "        q_new=q_new.replace('\"', '')\n",
    "        print (f\"\\n\")\n",
    "        conversation_turns.append(dict(name=question_gpt_name, text=q_new))\n",
    "    return conversation_turns, answer_agent.get_conv(), response, answer_agent\n",
    "\n",
    "def read_and_summarize_LlamaIndex( llm,\n",
    "                                  txt='This is a conversation.', q='',\n",
    "                                 ):\n",
    "    q=f\"\"\"Carefully read this conversation: \n",
    "\n",
    ">>>{txt}<<<\n",
    "\n",
    "Outline the final, refined procedure using the conversation as additional context.\n",
    "\n",
    "\"\"\" \n",
    "    summary, chat_engine=get_answer_LlamaIndex (llm, q=q,  \n",
    "                                 system_prompt=\"You analyze text and develop a clear and comprehensive final answer\",\n",
    "                                             max_new_tokens=5000)\n",
    "    return summary\n",
    "\n",
    "def answer_question_LlamaIndex ( #model, tokenizer, \n",
    "                    \n",
    "                      llm_answer,\n",
    "                    llm_question,   llm_summarize, \n",
    "    q='I have identified this amino acid sequence: AAAAAIIAAAA. How can I use it? ',\n",
    "                    bot_name_1=\"Biologist\",\n",
    "                    bot_instructions_1 = f\"\"\"You are a biologist. You are taking part in a discussion, from a life science perspective.\n",
    "Keep your answers brief, but accurate, and creative.\n",
    "\"\"\",\n",
    "                    bot_name_2=\"Engineer\",\n",
    "                    bot_instructions_2 = \"\"\"You are a critical engineer. You are taking part in a discussion, from the perspective of engineering.\n",
    "Keep your answers brief, and always challenge statements in a provokative way. As a creative individual, you inject ideas from other fields. \"\"\",\n",
    "                     include_N_turns_in_question_development=99999,\n",
    "                    total_turns=4,\n",
    "                    delete_last_question=True, #whether or not the last question is deleted (since it is not actually answered anyway)\n",
    "                    save_PDF=True,sample_question='',\n",
    "                    PDF_name=None,  save_dir='./', \n",
    "                     txt_file_path=None, marker_ch='>>> ',marker_ch_outer='### ',\n",
    "                     start_with_q=False,only_last=True,single_shot_question=True,\n",
    "                      messages_to_prompt=None,question_index=None, answer_index=None,chat_mode=\"context\",chat_token_limit=2500,\n",
    "                completion_to_prompt=None,te_on_question=False,verbose=False,\n",
    "                    ):\n",
    "\n",
    "    conversation_turns, answer_agent_conv, response, answer_agent = conversation_simulator_LlamaIndex( llm_answer, llm_question,#  model, tokenizer, \n",
    "                                                question_gpt_name=bot_name_2,answer_gpt_name=bot_name_1,\n",
    "                                             #   question_temperature=question_temperature,conv_temperature=conv_temperature,\n",
    "                                                question_asker_instructions=bot_instructions_2,\n",
    "                                                q=q, question_index=question_index, answer_index=answer_index,\n",
    "  include_N_turns_in_question_development=include_N_turns_in_question_development, \n",
    "    single_shot_question=single_shot_question,                                                                \n",
    "                                                total_turns=total_turns, data_dir=save_dir,marker_ch=marker_ch,marker_ch_outer=marker_ch_outer,\n",
    "                                                           start_with_q=start_with_q,only_last=only_last, sample_question=sample_question,\n",
    "                                                      verbose=verbose,chat_mode=chat_mode,chat_token_limit=chat_token_limit,\n",
    "                               #  messages_to_prompt=messages_to_prompt, completion_to_prompt=completion_to_prompt,\n",
    "                                                          )\n",
    "\n",
    "    if delete_last_question:\n",
    "        conversation_turns.pop() \n",
    "    txt=''\n",
    "    txt+=f\"The question discussed is: **{q.strip()}**\\n\\n\"\n",
    "     \n",
    "    print (\"-----------------------------------------\")\n",
    "    for turn in conversation_turns:\n",
    "        \n",
    "        txt +=f\"**{turn['name'].strip ()}**: {turn['text']}\\n\\n\"\n",
    "\n",
    "    summary = read_and_summarize_LlamaIndex(llm_summarize,#model, tokenizer ,\n",
    "                                                                txt, q=q,  )\n",
    "\n",
    "    integrated = f\"\"\"#### Question and conversation:\n",
    "    \n",
    "{txt} \n",
    "\n",
    "#### Summary:\n",
    "\n",
    "{summary}\n",
    "\n",
    "\"\"\"\n",
    "    if save_PDF:\n",
    "        html_text = markdown2.markdown(integrated)\n",
    "        max_len_fname=64\n",
    "        if PDF_name==None:\n",
    "            PDF_name=f'{save_dir}{q[:max_len_fname].strip()}.pdf'\n",
    "        \n",
    "        pdfkit.from_string(html_text, PDF_name)\n",
    "    max_len_fname=64\n",
    "    if txt_file_path==None:\n",
    "        txt_file_path = f'{save_dir}{q[:max_len_fname].strip()}.txt'\n",
    "    save_raw_txt=remove_markdown_symbols(integrated)\n",
    "    \n",
    "    with open(txt_file_path, 'w') as file:\n",
    "        file.write(save_raw_txt)\n",
    "\n",
    "    return conversation_turns, txt, summary, integrated, save_raw_txt, answer_agent_conv, response, answer_agent\n",
    "\n",
    "def get_entire_conversation_LlamaIndex (q, conversation_turns, marker_ch='### ', start_with_q=False, question_gpt_name='Question: '):\n",
    "    txt=''\n",
    "    if start_with_q:\n",
    "        txt+=f\"{marker_ch}The question discussed is: {q.strip()}\\n\\n\"\n",
    "    else:\n",
    "        txt=''\n",
    "    for turn in conversation_turns:\n",
    "        txt +=f\"{marker_ch}{turn['name'].strip ()}: {turn['text']}\\n\\n\"\n",
    "    return txt.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e2f89",
   "metadata": {},
   "source": [
    "# Load BioLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67844fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"https://huggingface.co/rachelkluu/Llama3.1-8b-Instruct-CPT-SFT-DPO-09022024-Q4_K_M-GGUF/resolve/main/llama3.1-8b-instruct-cpt-sft-dpo-09022024-q4_k_m.gguf\"\n",
    "\n",
    "biollm_agent = LlamaCPP(\n",
    "    model_url=model_url,\n",
    "    model_path=None,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=1024,\n",
    "    context_window=10000,\n",
    "    \n",
    "    generate_kwargs={\"top_k\":256, \"top_p\":0.9,\"repeat_penalty\":1.1,},\n",
    "     \n",
    "    model_kwargs={\"n_gpu_layers\": -1\n",
    "                 },\n",
    "\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af096bee",
   "metadata": {},
   "source": [
    "# Load Llama3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30e0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url= \"https://huggingface.co/rachelkluu/Llama-3.1-8B-Instruct-Q4_K_M-GGUF/resolve/main/llama-3.1-8b-instruct-q4_k_m.gguf\"\n",
    "\n",
    "llm_question = LlamaCPP(\n",
    "    model_url=model_url,\n",
    "    model_path=None,\n",
    "    temperature=0.3,\n",
    "    max_new_tokens=1024,\n",
    "    context_window=10000,\n",
    "    generate_kwargs={\"top_k\":256, \"top_p\":0.9,\"repeat_penalty\":1.1,},\n",
    "     \n",
    "    model_kwargs={\"n_gpu_layers\": -1,\n",
    "                 },\n",
    "   messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd2d777",
   "metadata": {},
   "source": [
    "## Load RAG For LLM Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436eda7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SimpleDirectoryReader(input_dir=\"./PlantPapers/\",).load_data()\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    data, transformations=[SentenceSplitter(chunk_size=256),],verbose=True, embed_model = embed_model ,\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817b240d",
   "metadata": {},
   "source": [
    "# Load JSON Q-A File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f76d9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output_string = \"\"\n",
    "jsonfile = \"rhapispollenpaper\" #name of saved  json file with generated Q-As\n",
    "\n",
    "with open(f'{jsonfile}.json', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        question = data.get('Question', 'No question found')\n",
    "        answer = data.get('Answer', 'No answer found')\n",
    "        output_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "\n",
    "# Print the full output string to check\n",
    "print(output_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f88e76",
   "metadata": {},
   "source": [
    "## Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc69cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='./convo_out/'\n",
    "make_dir_if_needed(data_dir)\n",
    "\n",
    "prompt = \"Design a procedure to make rhapis excelsa leaf paper, inspired by the way pollen paper was made.\"\n",
    "finalquery = f\"\"\"{prompt}\n",
    "\n",
    "Here is some additional information that can support this task: \n",
    "{output_string}\"\"\"\n",
    "\n",
    "##########################################\n",
    "# First bot is the expert being asked\n",
    "##########################################\n",
    "bot_name_1=\"BioinspiredLLM\"\n",
    "bot_instructions_1 = f\"\"\"You are a {bot_name_1}, a creative engineer with knowledge in biological and bio-inspired materials.\n",
    "\n",
    "You are taking part in a discussion and have access to potentially relevant information from previous papers on Rhapis excelsa leaves and pollen-based materials. \n",
    "\n",
    "However, be cautious and do not cite specific studies or references. Focus on providing concise, accurate, and thoughtful contributions that stimulate new trains of thought.\n",
    "\"\"\"\n",
    "##########################################\n",
    "# Second bot asks questions in an adversarial way\n",
    "##########################################\n",
    "bot_name_2=\"Scientist\"\n",
    "bot_instructions_2 = f\"\"\"You are a {bot_name_2}, taking part in a discussion where your role is to collaborate with BioinspiredLLM to develop a scientific procedure.\n",
    "\n",
    "Prioritize scientific effectivenes and logical reasoning when providing potential implications or improvements.\n",
    "\n",
    "You have access to potentially relevant information from previous papers on Rhapis excelsa leaves and pollen-based materials. \"\"\"\n",
    "\n",
    "sample_question=''\n",
    "\n",
    "total_turns=3\n",
    "include_N_turns_in_question_development=total_turns\n",
    "##########################################\n",
    "# Let's begin\n",
    "##########################################\n",
    "conversation_turns, txt, summary, integrated, save_raw_txt, answer_agent_conv, response, answer_agent = answer_question_LlamaIndex (# model, tokenizer,\n",
    "                                                                                                            q=finalquery,\n",
    "                llm_answer=biollm_agent,\n",
    "                    llm_question=llm_question,  \n",
    "                        llm_summarize=llm_question,\n",
    "                    bot_name_1=bot_name_1,\n",
    "                    bot_instructions_1 = bot_instructions_1,\n",
    "                    bot_name_2=bot_name_2,\n",
    "                    bot_instructions_2 = bot_instructions_2,\n",
    "                    total_turns=total_turns,\n",
    "                    delete_last_question=True,\n",
    "                    save_dir=data_dir, marker_ch='>>>',marker_ch_outer='###',\n",
    "    start_with_q=False,only_last=False,\n",
    "        question_index=index, answer_index=index,\n",
    "    verbose=False, \n",
    "    chat_mode=\"context\",chat_token_limit=5000,single_shot_question=True,\n",
    "    sample_question=sample_question,include_N_turns_in_question_development=include_N_turns_in_question_development,\n",
    "                    )\n",
    "\n",
    "display(Markdown(integrated))\n",
    "txt=remove_markdown_symbols(integrated)\n",
    "max_len_fname=64\n",
    "txt_file_path = f'{data_dir}{finalquery[:max_len_fname].strip()}.txt'\n",
    "save_raw_txt=remove_markdown_symbols(integrated)\n",
    "\n",
    "with open(txt_file_path, 'w') as file:\n",
    "    file.write(save_raw_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01f8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plantmat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
